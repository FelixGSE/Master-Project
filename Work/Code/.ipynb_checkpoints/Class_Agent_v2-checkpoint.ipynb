{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import numpy as np\n",
    "import os \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bandit:\n",
    "    \n",
    "    def __init__( self, mu = [0,0] , sigma = [1,1], N = 10 , seed = None ):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.N = N\n",
    "        self.seed = seed\n",
    "        if self.seed == None:\n",
    "            self.bandits = self.create_bandits(self.mu,self.sigma,self.N)\n",
    "        else:\n",
    "            rd.seed( self.seed )\n",
    "            self.bandits = self.create_bandits(self.mu,self.sigma,self.N)\n",
    "    \n",
    "    def rnorm(self, mu, sigma, N ):\n",
    "        number = np.random.normal(mu, sigma, N)\n",
    "        result = number.tolist()\n",
    "        return result\n",
    "        \n",
    "    def create_bandits(self, mu ,sigma, N ):\n",
    "        n_bandit = len( mu )\n",
    "        bandits = []\n",
    "        for i in range( n_bandit ):\n",
    "            number = self.rnorm( mu[i] , sigma[i] , N )\n",
    "            bandits.append(number)\n",
    "        return bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Initialize class agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class agent:\n",
    "    \n",
    "    \"\"\" \n",
    "    SET INITIAL CONDITIONS\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, alpha = 0.5, epsilon = 0.01, tau = 0.1 , decision_function = \"softmax\", \\\n",
    "                  reward_input = None, mu = [0,0] , sigma = [1,1], init = None, time = 10, save = False ):\n",
    "        \n",
    "        # PARAMETERS\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        \n",
    "        # DECISION FUNCTION\n",
    "        self.decision_function = decision_function\n",
    "        \n",
    "        # BANDITS\n",
    "        if reward_input == None:\n",
    "            self.mu = mu\n",
    "            self.sigma = sigma\n",
    "            self.reward_set = None\n",
    "            self.n_bandits = len( self.mu )\n",
    "        else:  \n",
    "            self.reward_set = reward_input\n",
    "            self.n_bandits = len( self.reward_set )\n",
    "            \n",
    "        # VALUE FUNCTION, REWARDS AND CHOICE LISTS\n",
    "        if init == None: \n",
    "            self.value_function = self.init_value_container(self.n_bandits)\n",
    "        else:\n",
    "            self.value_function = self.init_value_container(self.n_bandits,self.init)\n",
    "        self.choices = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        # OTHER \n",
    "        self.time = time\n",
    "        self.save = save\n",
    "    \n",
    "    \n",
    "    \"\"\" \n",
    "    LEARNING FUNCTIONS\n",
    "    \"\"\"\n",
    "    \n",
    "    # Softmax-Decision - Returns the next choice based on boltzman distrubution\n",
    "    def softmax(self, value_array, tau):\n",
    "        # Check for correct input specification \n",
    "        if tau == 0:\n",
    "            print \"ERROR: Parameter tau can't be zero!\"\n",
    "            return None\n",
    "        bandits = range(self.n_bandits)\n",
    "        numerator = np.exp( np.array( value_array ) ) / float( tau )\n",
    "        denominator = sum( numerator )\n",
    "        boltzman_distribution = numerator / denominator\n",
    "        choice = self.weighted_sample(bandits,probability = boltzman_distribution)\n",
    "        return choice\n",
    "    \n",
    "    # Epsilon-Greedy-Decision - Returns greedy vs. random choice\n",
    "    def epsilon_greedy(self, value_array, probability ):\n",
    "        choice_count = len( value_array )\n",
    "        explore = random_bool( p = probability )\n",
    "        if explore == 1:\n",
    "            choice = np.random.choice(choice_count)\n",
    "            return choice\n",
    "        else:\n",
    "            optimal = value_array.argmax()\n",
    "            return optimal        \n",
    "        \n",
    "    \"\"\" \n",
    "    LEARNING FUNCTION\n",
    "    \"\"\"\n",
    "    \n",
    "    # Central function to run learning procedure of class agent\n",
    "    def learn(self, run_time = None ):\n",
    "         \n",
    "        # Control run time and initilize bandits \n",
    "        if run_time == None:\n",
    "            run_time = self.time\n",
    "        if self.reward_set == None:\n",
    "            bandits = self.create_bandits( mu = self.mu, sigma = self.sigma, N = run_time )\n",
    "        else:\n",
    "            bandits = self.reward_set\n",
    "        if  self.reward_set is not None:\n",
    "            run_time = len( self.reward_set[0] )\n",
    "            print \"WARNING: Note that run time was fixed to length of bandits\"\n",
    "\n",
    "        # Run learning procedure for option softmax\n",
    "        if self.decision_function == \"softmax\":\n",
    "            \n",
    "            for step in range( run_time ):\n",
    "                \n",
    "                # Choose next action and reward\n",
    "                states = self.current_values_lookup( self.value_function )\n",
    "                current_decision = self.softmax( value_array = states , tau = self.tau)\n",
    "                current_reward = bandits[current_decision][step]\n",
    "                \n",
    "                # Update value function and store decision and rewards \n",
    "                self.choices.append( current_decision )\n",
    "                self.rewards.append( current_reward )\n",
    "                self.update_value_functions( current_decision, current_reward, self.alpha  )\n",
    "        \n",
    "        # Run learning procedure for option epsilon greedy \n",
    "        if self.decision_function == \"epsgreedy\":\n",
    "         \n",
    "            for step in range( run_time ):\n",
    "                \n",
    "                # Choose next action and reward\n",
    "                states = self.current_values_lookup(self.value_function)\n",
    "                decision = self.epsilon_greedy( value_array = states , tau = self.epsilon)\n",
    "                current_reward = bandits[current_decision][step]\n",
    "                \n",
    "                # Update value function and store decision and rewards \n",
    "                self.choices.append( current_decision )\n",
    "                self.rewards.append( current_reward )\n",
    "                self.update_value_functions( current_decision, current_reward, self.alpha  )\n",
    "        \n",
    "    \"\"\" \n",
    "    AUXILLIARY FUNCTIONS\n",
    "    \"\"\"\n",
    "    \n",
    "    # Wrapper function for sampling randomly TRUE or FALSE with probability p\n",
    "    def random_bool(self, p = 0.1 ):\n",
    "        number = np.random.binomial( 1 , p, 1 )\n",
    "        result = number.tolist()[0] \n",
    "        return result\n",
    "\n",
    "    # Wrapper for sampling N normal random numbers with mean mu and variance sigma\n",
    "    def rnorm(self, mu, sigma, N ):\n",
    "        number = np.random.normal(mu, sigma, N)\n",
    "        result = number.tolist()\n",
    "        return result\n",
    "    \n",
    "    # Wrapper for sampling a random number from an array with corresponding probabilities\n",
    "    def weighted_sample(self, items, probability ):\n",
    "        number = np.random.choice(items,1,p=probability)\n",
    "        rchoice = number.tolist()[0]\n",
    "        return rchoice   \n",
    "    \n",
    "    # Inititialize n value storages 0 or specified initial values\n",
    "    def init_value_container(self, vfcount , init_val = None ):\n",
    "        max_itter = vfcount\n",
    "        value_functions = []\n",
    "        if init_val == None:\n",
    "            init_val = [0] * max_itter\n",
    "        for i in range( max_itter ):\n",
    "            temp_val = init_val[i]\n",
    "            temp_list = [temp_val]\n",
    "            value_functions.append(temp_list)\n",
    "        return value_functions\n",
    "    \n",
    "    # Creates normally distributed N-arm bandits \n",
    "    def create_bandits(self, mu ,sigma, N ):\n",
    "        # Check input parameters\n",
    "        if len(mu) != len(sigma):\n",
    "            print \"Unsufficient parameter input\"\n",
    "            return None\n",
    "        # define how many bandits to compute\n",
    "        n_bandit = len( mu )\n",
    "        bandits = []\n",
    "        for i in range(n_bandit):\n",
    "            number = self.rnorm( mu[i] , sigma[i] , N )\n",
    "            bandits.append(number)\n",
    "        return bandits\n",
    "    \n",
    "    # Wrapper for looking up the last value in each value_function ( == last column of list of list )\n",
    "    def current_values_lookup(self, listoflist ):\n",
    "        cols = len(listoflist[0]) - 1\n",
    "        last_value = [row[cols] for row in listoflist]\n",
    "        return last_value\n",
    "    \n",
    "    # Update value functions according to choices rewards and alpha\n",
    "    def update_value_functions(self, choice, reward, alpha ):\n",
    "        for i in range(  self.n_bandits  ):\n",
    "            current_value_function = self.value_function[i]\n",
    "            old_value = current_value_function[-1]\n",
    "            if i == choice:\n",
    "                new_value = old_value + alpha * ( reward - old_value )\n",
    "            else:\n",
    "                new_value = old_value\n",
    "            \n",
    "            current_value_function.append(new_value)\n",
    "        \n",
    "    \"\"\" \n",
    "    BACK UP AND RESET\n",
    "    \"\"\"\n",
    "        \n",
    "    # Save current state of the value function, choices and experienced rewards                  \n",
    "    def save_history( self , path = None ):\n",
    "        \n",
    "        # Define file names for storage objects for softmax decision function\n",
    "        if self.decision_function == \"softmax\":\n",
    "            str_alpha = \"alpha_\" + str(self.alpha)\n",
    "            str_tau = \"_tau_\" + str(self.tau)\n",
    "            file_type = \".txt\"\n",
    "            sufix = str_alpha + str_tau + file_type\n",
    "            value_name = \"valuefunction_\" + sufix\n",
    "            choice_name = \"choices\" + sufix\n",
    "            reward_name = \"reward\" + sufix\n",
    "        \n",
    "        # Define file names for storage objects for epsilon greedy decision function \n",
    "        if self.decision_function == \"epsgreedy\":\n",
    "            # Prepare File name - Parameters\n",
    "            str_alpha = \"alpha_\" + str(self.alpha)\n",
    "            str_epsilon = \"_epsilon_\" + str(self.epsilon)\n",
    "            file_type = \".txt\"\n",
    "            sufix = str_alpha + str_epsilon + file_type\n",
    "            value_name = \"valuefunction_\" + sufix\n",
    "            choice_name = \"choices\" + sufix\n",
    "            reward_name = \"reward\" + sufix  \n",
    "        \n",
    "        # Combine file names and path\n",
    "        if path == None:\n",
    "            path = os.getcwd() + \"/\"\n",
    "        value_file = path + value_name\n",
    "        choice_file = path + choice_name\n",
    "        reward_file = path + reward_name\n",
    "        \n",
    "        # Save value functions, choice- and reward lists\n",
    "        json.dump(self.value_function, file(value_file, 'w'))\n",
    "        json.dump(self.choices, file(choice_file, 'w'))\n",
    "        json.dump(self.rewards, file(reward_file, 'w'))\n",
    "    \n",
    "    # Clear value function of the agent      \n",
    "    def re_init( self, init = None ):\n",
    "        if init == None: \n",
    "            self.value_function = self.init_value_container(self.n_bandits)\n",
    "        else:\n",
    "            self.value_function = self.init_value_container(self.n_bandits,self.init)\n",
    "        self.choices = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Note that run time was fixed to length of bandits\n",
      "[[0, 0, 0, 0], [0, 2.0, 3.5, 4.75]]\n",
      "[1, 1, 1]\n",
      "[4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# Intitialize agent\n",
    "new_guy = agent( reward_input = [[1,2,3],[4,5,6]], decision_function = \"softmax\" )\n",
    "new_guy.learn(100)\n",
    "print new_guy.value_function\n",
    "print new_guy.choices\n",
    "print new_guy.rewards\n",
    "\n",
    "# Show the developement of the value function for each bandit\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

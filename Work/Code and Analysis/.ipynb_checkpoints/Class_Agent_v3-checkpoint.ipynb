{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import numpy as np\n",
    "import os \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2) Class Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bandit:\n",
    "    \n",
    "    \"\"\" \n",
    "    SET INITIAL CONDITIONS\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, mu = [0,0] , sigma = [1,1], N = 10 , seed = None ):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.N = N\n",
    "        self.seed = seed\n",
    "        if self.seed == None:\n",
    "            self.bandits = self.create_bandits(self.mu,self.sigma,self.N)\n",
    "        else:\n",
    "            rd.seed( self.seed )\n",
    "            self.bandits = self.create_bandits(self.mu,self.sigma,self.N)\n",
    "    \n",
    "    \"\"\" \n",
    "    Function to create bandits\n",
    "    \"\"\"\n",
    "    \n",
    "    def create_bandits(self, mu ,sigma, N ):\n",
    "        n_bandit = len( mu )\n",
    "        bandits = []\n",
    "        for i in range( n_bandit ):\n",
    "            number = self.rnorm( mu[i] , sigma[i] , N )\n",
    "            bandits.append(number)\n",
    "        return bandits\n",
    "    \n",
    "    \"\"\" \n",
    "    Auxilliary functions\n",
    "    \"\"\"\n",
    "    \n",
    "    def rnorm(self, mu, sigma, N ):\n",
    "        number = np.random.normal(mu, sigma, N)\n",
    "        result = number.tolist()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Initialize class agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class agent:\n",
    "    \n",
    "    \"\"\" \n",
    "    SET INITIAL CONDITIONS\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__( self, alpha = 0.5, epsilon = 0.01, tau = 0.1 , decision_function = \"softmax\", \\\n",
    "                  reward_input = None, mu = [0,0] , sigma = [1,1], seed = None, init = None, time = 10, save = False ):\n",
    "        \n",
    "        # PARAMETERS\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        \n",
    "        # DECISION FUNCTION\n",
    "        self.decision_function = decision_function\n",
    "        \n",
    "        # BANDITS\n",
    "        if reward_input == None:\n",
    "            self.mu = mu\n",
    "            self.sigma = sigma\n",
    "            self.reward_set = None\n",
    "            self.n_bandits = len( self.mu )\n",
    "            self.seed = seed\n",
    "        else:  \n",
    "            self.reward_set = reward_input\n",
    "            self.n_bandits = len( self.reward_set )\n",
    "            \n",
    "        # VALUE FUNCTION, REWARDS AND CHOICE LISTS\n",
    "        if init == None: \n",
    "            self.value_function = self.init_value_container(self.n_bandits)\n",
    "        else:\n",
    "            self.value_function = self.init_value_container(self.n_bandits,self.init)\n",
    "        self.choices = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        # OTHER \n",
    "        self.time = time\n",
    "        self.save = save\n",
    "    \n",
    "    \n",
    "    \"\"\" \n",
    "    LEARNING FUNCTIONS\n",
    "    \"\"\"\n",
    "    \n",
    "    # Softmax-Decision - Returns the next choice based on boltzman distrubution\n",
    "    def softmax(self, value_array, tau):\n",
    "        # Check for correct input specification \n",
    "        if tau == 0:\n",
    "            print \"ERROR: Parameter tau can't be zero!\"\n",
    "            return None\n",
    "        bandits = range(self.n_bandits)\n",
    "        numerator = np.exp( np.array( value_array ) ) / float( tau )\n",
    "        denominator = sum( numerator )\n",
    "        boltzman_distribution = numerator / denominator\n",
    "        choice = self.weighted_sample(bandits,probability = boltzman_distribution)\n",
    "        return choice\n",
    "    \n",
    "    # Epsilon-Greedy-Decision - Returns greedy vs. random choice\n",
    "    def epsilon_greedy(self, value_array, probability ):\n",
    "        choice_count = len( value_array )\n",
    "        explore = random_bool( p = probability )\n",
    "        if explore == 1:\n",
    "            choice = np.random.choice(choice_count)\n",
    "            return choice\n",
    "        else:\n",
    "            optimal = value_array.argmax()\n",
    "            return optimal        \n",
    "        \n",
    "    \"\"\" \n",
    "    LEARNING FUNCTION\n",
    "    \"\"\"\n",
    "    \n",
    "    # Central function to run learning procedure of class agent\n",
    "    def learn(self, run_time = None ):\n",
    "         \n",
    "        # Control run time and initilize bandits \n",
    "        if run_time == None:\n",
    "            run_time = self.time\n",
    "        if self.reward_set == None:\n",
    "            raw = bandit( mu = self.mu, sigma = self.sigma, N = run_time, seed = self.seed )\n",
    "            bandits = raw.bandits\n",
    "        else:\n",
    "            bandits = self.reward_set\n",
    "        if  self.reward_set is not None:\n",
    "            run_time = len( self.reward_set[0] )\n",
    "            print \"WARNING: Note that run time was fixed to length of bandits\"\n",
    "\n",
    "        # Run learning procedure for option softmax\n",
    "        if self.decision_function == \"softmax\":\n",
    "            \n",
    "            for step in range( run_time ):\n",
    "                \n",
    "                # Choose next action and reward\n",
    "                states = self.current_values_lookup( self.value_function )\n",
    "                current_decision = self.softmax( value_array = states , tau = self.tau)\n",
    "                current_reward = bandits[current_decision][step]\n",
    "                \n",
    "                # Update value function and store decision and rewards \n",
    "                self.choices.append( current_decision )\n",
    "                self.rewards.append( current_reward )\n",
    "                self.update_value_functions( current_decision, current_reward, self.alpha  )\n",
    "        \n",
    "        # Run learning procedure for option epsilon greedy \n",
    "        if self.decision_function == \"epsgreedy\":\n",
    "         \n",
    "            for step in range( run_time ):\n",
    "                \n",
    "                # Choose next action and reward\n",
    "                states = self.current_values_lookup(self.value_function)\n",
    "                decision = self.epsilon_greedy( value_array = states , tau = self.epsilon)\n",
    "                current_reward = bandits[current_decision][step]\n",
    "                \n",
    "                # Update value function and store decision and rewards \n",
    "                self.choices.append( current_decision )\n",
    "                self.rewards.append( current_reward )\n",
    "                self.update_value_functions( current_decision, current_reward, self.alpha  )\n",
    "        \n",
    "    \"\"\" \n",
    "    AUXILLIARY FUNCTIONS\n",
    "    \"\"\"\n",
    "    \n",
    "    # Wrapper function for sampling randomly TRUE or FALSE with probability p\n",
    "    def random_bool(self, p = 0.1 ):\n",
    "        number = np.random.binomial( 1 , p, 1 )\n",
    "        result = number.tolist()[0] \n",
    "        return result\n",
    "\n",
    "    # Wrapper for sampling a random number from an array with corresponding probabilities\n",
    "    def weighted_sample(self, items, probability ):\n",
    "        number = np.random.choice(items,1,p=probability)\n",
    "        rchoice = number.tolist()[0]\n",
    "        return rchoice   \n",
    "    \n",
    "    # Inititialize n value storages 0 or specified initial values\n",
    "    def init_value_container(self, vfcount , init_val = None ):\n",
    "        max_itter = vfcount\n",
    "        value_functions = []\n",
    "        if init_val == None:\n",
    "            init_val = [0] * max_itter\n",
    "        for i in range( max_itter ):\n",
    "            temp_val = init_val[i]\n",
    "            temp_list = [temp_val]\n",
    "            value_functions.append(temp_list)\n",
    "        return value_functions\n",
    "\n",
    "    # Wrapper for looking up the last value in each value_function ( == last column of list of list )\n",
    "    def current_values_lookup(self, listoflist ):\n",
    "        cols = len(listoflist[0]) - 1\n",
    "        last_value = [row[cols] for row in listoflist]\n",
    "        return last_value\n",
    "    \n",
    "    # Update value functions according to choices rewards and alpha\n",
    "    def update_value_functions(self, choice, reward, alpha ):\n",
    "        for i in range(  self.n_bandits  ):\n",
    "            current_value_function = self.value_function[i]\n",
    "            old_value = current_value_function[-1]\n",
    "            if i == choice:\n",
    "                new_value = old_value + alpha * ( reward - old_value )\n",
    "            else:\n",
    "                new_value = old_value\n",
    "            \n",
    "            current_value_function.append(new_value)\n",
    "        \n",
    "    \"\"\" \n",
    "    BACK UP AND RESET\n",
    "    \"\"\"\n",
    "        \n",
    "    # Save current state of the value function, choices and experienced rewards                  \n",
    "    def save_history( self , path = None ):\n",
    "        \n",
    "        # Define file names for storage objects for softmax decision function\n",
    "        if self.decision_function == \"softmax\":\n",
    "            str_alpha = \"alpha_\" + str(self.alpha)\n",
    "            str_tau = \"_tau_\" + str(self.tau)\n",
    "            file_type = \".txt\"\n",
    "            sufix = str_alpha + str_tau + file_type\n",
    "            value_name = \"valuefunction_\" + sufix\n",
    "            choice_name = \"choices\" + sufix\n",
    "            reward_name = \"reward\" + sufix\n",
    "        \n",
    "        # Define file names for storage objects for epsilon greedy decision function \n",
    "        if self.decision_function == \"epsgreedy\":\n",
    "            # Prepare File name - Parameters\n",
    "            str_alpha = \"alpha_\" + str(self.alpha)\n",
    "            str_epsilon = \"_epsilon_\" + str(self.epsilon)\n",
    "            file_type = \".txt\"\n",
    "            sufix = str_alpha + str_epsilon + file_type\n",
    "            value_name = \"valuefunction_\" + sufix\n",
    "            choice_name = \"choices\" + sufix\n",
    "            reward_name = \"reward\" + sufix  \n",
    "        \n",
    "        # Combine file names and path\n",
    "        if path == None:\n",
    "            path = os.getcwd() + \"/\"\n",
    "        value_file = path + value_name\n",
    "        choice_file = path + choice_name\n",
    "        reward_file = path + reward_name\n",
    "        \n",
    "        # Save value functions, choice- and reward lists\n",
    "        json.dump(self.value_function, file(value_file, 'w'))\n",
    "        json.dump(self.choices, file(choice_file, 'w'))\n",
    "        json.dump(self.rewards, file(reward_file, 'w'))\n",
    "    \n",
    "    # Clear value function of the agent      \n",
    "    def re_init( self, init = None ):\n",
    "        if init == None: \n",
    "            self.value_function = self.init_value_container(self.n_bandits)\n",
    "        else:\n",
    "            self.value_function = self.init_value_container(self.n_bandits,self.init)\n",
    "        self.choices = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.2725659852109284, 0.2725659852109284, 0.28511803014825665, 0.28511803014825665, 0.28511803014825665, 0.28511803014825665, 0.28511803014825665, 0.5138335114506427, -0.2352500977245917, -0.12899260995130937, 0.7137962911531474, 0.8602297329900761, 0.8602297329900761, 0.6164659811320252, 1.1000530023903852, 0.7065962699071017, 0.7065962699071017, -0.11022342984748901, -0.6454599038651646, -0.6454599038651646, -0.0996430892908945, -0.0996430892908945, -0.15660115668436042, 0.5289751347684943, -0.062684240834189, -0.08071177071732905, -0.08071177071732905, -0.08071177071732905, -0.08071177071732905, -0.08071177071732905, -0.08071177071732905, -0.11554055090322486, 0.06344601177447638, -0.7181860312616086, -0.7181860312616086, 0.013400220682818986, 0.14242605730291744, -0.7759899033231988, -0.7759899033231988, -0.9803408255336372, -0.9803408255336372, -0.9803408255336372, -0.7778941118977442, -0.7778941118977442, -0.7778941118977442, -0.7778941118977442, -1.0148801613196172, -1.0148801613196172, -1.0148801613196172, -0.31198981495510336, -0.31198981495510336, -0.31198981495510336, -0.31198981495510336, -0.31198981495510336, -0.31198981495510336, -0.31198981495510336, -0.640075493725807, -0.640075493725807, -0.640075493725807, -0.640075493725807, -0.640075493725807, -0.30255459653636774, -0.30255459653636774, -0.30255459653636774, -0.30255459653636774, -0.30255459653636774, -0.30255459653636774, -0.24989467225555134, 0.7801600618517763, 0.7482815242227351, 0.7192047029064708, 0.7192047029064708, 0.37547352198803646, 0.37547352198803646, 0.37547352198803646, 0.2254994401774968, 0.2254994401774968, -0.44051397841537443, -0.44051397841537443, -0.44051397841537443, 0.018854633089790185, 0.018854633089790185, 0.5035992678966379, -0.2795892700208429, -0.2795892700208429, -0.13957385178374132, 0.03409625415130785, -0.05438967971179791, -0.05438967971179791, -0.05438967971179791, -0.05438967971179791, -0.02012479957798953, -0.8705596232657914, -0.8705596232657914, -1.599698304390507, -1.599698304390507, -1.599698304390507, -1.599698304390507, -1.599698304390507, -1.599698304390507], [0, 0, -0.028279724720972427, -0.028279724720972427, -0.08452897916465921, 0.2230118501971269, -0.11277141277959099, -0.34774795854532625, -0.34774795854532625, -0.34774795854532625, -0.34774795854532625, -0.34774795854532625, -0.34774795854532625, -0.36995340385918535, -0.36995340385918535, -0.36995340385918535, -0.36995340385918535, -0.5412640470463302, -0.5412640470463302, -0.5412640470463302, -0.7182222190496708, -0.7182222190496708, -0.05064774126591043, -0.05064774126591043, -0.05064774126591043, -0.05064774126591043, -0.05064774126591043, -0.2899284118732877, 0.13365451782056076, -0.32424788423661016, 0.0037756976463622083, 0.4873416815159451, 0.4873416815159451, 0.4873416815159451, 0.4873416815159451, 0.5659869751261575, 0.5659869751261575, 0.5659869751261575, 0.5659869751261575, 0.4312916733541173, 0.4312916733541173, -0.174090806501558, -0.36453278913687737, -0.36453278913687737, -0.7892321930282311, -0.3085835502525757, 0.13024229965685757, 0.13024229965685757, -0.4580595424096113, 0.01537737347037077, 0.01537737347037077, -0.35719167966370763, 0.7929503183947006, 0.25952780277148435, 0.4795690823994188, 0.7210920970657969, -0.2659084908619863, -0.2659084908619863, -0.2839567021565592, 0.4682922169775354, 0.11838264715228786, -0.056005478286911325, -0.056005478286911325, 0.07496137047129964, 1.0478959774022072, 0.8728153394728277, -0.07160137776267272, -0.6143796959074078, -0.6143796959074078, -0.6143796959074078, -0.6143796959074078, -0.6143796959074078, 0.3360068246118256, 0.3360068246118256, -0.1383886701357266, -0.6415797609228124, -0.6415797609228124, -0.1624417166122647, -0.1624417166122647, -0.22299490972222344, 0.5164791380853849, 0.5164791380853849, -0.32808048300962966, -0.32808048300962966, -0.32808048300962966, -0.014568410555783617, -0.014568410555783617, -0.014568410555783617, -0.014568410555783617, 1.0451091543306785, 0.5483527586993087, -0.863990600628112, -0.863990600628112, -0.863990600628112, -0.6575171098343099, -0.6575171098343099, -0.4717346997844243, 0.2734721055109366, 0.401350573113355, 0.6942930175332369, 0.8862088074014879]]\n",
      "[0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1]\n",
      "[0.5451319704218568, -0.05655944944194485, 0.29767007508558496, -0.140778233608346, 0.530552679558913, -0.44855467575630886, -0.5827245043110615, 0.7425489927530289, -0.9843337068998261, -0.02273512217802704, 1.556585192257604, 1.006663174827005, -0.39215884917304444, 0.37270222927397445, 1.5836400236487453, 0.31313953742381817, -0.7125746902334752, -0.9270431296020798, -1.18069637788284, -0.8951803910530114, 0.44617372528337557, 0.6169267365178499, -0.21355922407782632, 1.214551426221349, -0.6543436164368724, -0.09873930060046911, -0.529209082480665, 0.5572374475144092, -0.782150286293781, 0.3317992795293346, 0.970907665385528, -0.15036933108912068, 0.24243257445217764, -1.4998180742976936, 0.6446322687363698, 0.7449864726272466, 0.2714518939230159, -1.6944058639493151, 0.2965963715820771, -1.1846917477440755, -0.7794732863572332, -0.5549747717721967, -0.5754473982618511, -1.213931596919585, 0.1720650925230797, 0.5690681495662908, -1.2518662107414904, -1.0463613844760802, 0.48881428935035287, 0.39090053140941045, -0.7297607327977861, 1.9430923164531086, -0.2738947128517319, 0.6996103620273533, 0.9626151117321751, -1.2529090787897696, -0.9681611724965106, -0.3020049134511322, 1.22054113611163, -0.23152692267295977, -0.23039360372611054, 0.034966300653071485, 0.20592821922951057, 2.020830584333115, 0.6977347015434482, -1.0160180949981732, -1.157158014052143, -0.19723474797473495, 1.8102147959591037, 0.7164029865936938, 0.6901278815902065, 1.286393345131059, 0.03174234106960219, -0.6127841648832788, -1.1447708517098985, 0.07552535836695709, 0.3166963276982831, -1.1065273970082457, -0.2835481028321822, 1.2559531858929933, 0.47822324459495474, -1.1726401041046444, 0.9883439027034855, -1.0627778079383237, 0.29894366189806243, 0.00044156645336023976, 0.20776636008635704, -0.14287561357490366, 2.104786719217141, 0.051596363067938876, -2.2763339599555326, 0.014140080555818843, -1.7209944469535934, -0.4510436190405079, -2.3288369855152227, -0.28595228973453873, 1.0186789108062975, 0.5292290407157734, 0.9872354619531187, 1.0781245972697389]\n"
     ]
    }
   ],
   "source": [
    "# Intitialize agent\n",
    "new_guy = agent(  decision_function = \"softmax\" )\n",
    "new_guy.learn(100)\n",
    "print new_guy.value_function\n",
    "print new_guy.choices\n",
    "print new_guy.rewards\n",
    "\n",
    "# Show the developement of the value function for each bandit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

% ----------------------------------------------------------------------------------------------------------
% Packages
% ----------------------------------------------------------------------------------------------------------

\documentclass[12pt,a4paper,bibliography=totocnumbered,listof=totocnumbered]{scrartcl}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{tabularx}
\usepackage{geometry}
\usepackage{setspace}
\usepackage[right]{eurosym}
\usepackage[printonlyused]{acronym}
\usepackage{subfig}
\usepackage{floatflt}
\usepackage[usenames,dvipsnames]{color}
\usepackage{colortbl}
\usepackage{paralist}
\usepackage{array}
\usepackage{titlesec}
%\usepackage{dsfont}
\usepackage{parskip}
\usepackage[right]{eurosym}
\usepackage[subfigure,titles]{tocloft}
\usepackage[pdfpagelabels=true]{hyperref}
\usepackage{mathdots}
\usepackage{listings}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{fix-cm}
\usepackage{rotating}
\usepackage{pdflscape}
\usepackage{bbold}
\usepackage[labelfont=bf]{caption}
\captionsetup{labelfont=bf}
\usepackage{tikz}
\usepackage{breakcites}
\usepackage{float}
\usepackage{tcolorbox}
 \usepackage[flushleft]{threeparttable}
\usepackage{tocbibind}
\usepackage{algorithm2e}
\newcommand{\listofalgorithmes}{\tocfile{\listalgorithmcfname}{loa}}

%---------------------------------------------------------------------------------------------------------
% Packages
% ----------------------------------------------------------------------------------------------------------


\lstset{basicstyle=\footnotesize, captionpos=t, breaklines=true, showstringspaces=false, tabsize=2, frame=lines, numbers=left, numberstyle=\tiny, xleftmargin=2em, framexleftmargin=2em}
\makeatletter
\def\l@lstlisting#1#2{\@dottedtocline{1}{0em}{1em}{\hspace{1,5em} Lst. #1}{#2}}
\makeatother



\geometry{a4paper, top=27mm, left=27mm, right=27mm, bottom=35mm, headsep=10mm, footskip=12mm}

% ----------------------------------------------------------------------------------------------------------
% Packages
% ----------------------------------------------------------------------------------------------------------


\hypersetup{unicode=false, pdftoolbar=true, pdfmenubar=true, pdffitwindow=false, pdfstartview={FitH},
	pdftitle={Master Thesis},
	pdfauthor={Felix Gutmann},
	pdfsubject={Bachelor Thesis},
	pdfcreator={\LaTeX\ with package \flqq hyperref\frqq},
	pdfproducer={pdfTeX \the\pdftexversion.\pdftexrevision},
	pdfkeywords={Bachelor Thesis},
	pdfnewwindow=true,
	colorlinks=true,linkcolor=black,citecolor=black,filecolor=magenta,urlcolor=black}
\pdfinfo{/CreationDate (D:20110620133321)}
\DeclareMathOperator*{\argmin}{arg\,min}



\begin{document}

\titlespacing\section{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsection{0pt}{30pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
\titlespacing\subsubsection{0pt}{12pt plus 4pt minus 2pt}{0pt plus 2pt minus 2pt}
% Headers and footers

\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\leftmark}{\rightmark}
\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{\thesection\space\contentsname}
%\lfoot{Complex Economic Systems - An analytical approach to Input-Output tables\newline}
\cfoot{}
\rfoot{\ \linebreak \thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

% ----------------------------------------------------------------------------------------------------------
%Prefix
% ----------------------------------------------------------------------------------------------------------

\renewcommand{\thesection}{\Roman{section}}
\renewcommand{\theHsection}{\Roman{section}}
\pagenumbering{Roman}

% ----------------------------------------------------------------------------------------------------------
% Title
% ----------------------------------------------------------------------------------------------------------

\thispagestyle{empty}
\begin{center}
	\includegraphics[width=\textwidth]{Pictures/logo01.jpg}\\
	\vspace*{2cm}
	\vspace*{2cm}
	\Huge
	\textbf{Master Thesis}\\
	\vspace*{0.5cm}
	\large
	\textbf{Topic:}\\
	\vspace*{1cm}
	\textbf{Unsupervised learning in decision making}\\
	\vspace*{2cm}
\end{center}	

$\vspace{6cm}$
\begin{tabbing}
\hspace*{1cm}\=\hspace*{3.2cm}\=\hspace*{3cm}\=\hspace*{2.7cm}\= \kill
\onehalfspacing
\textbf{Author:} \>\> Domagoj Fizulic\\
\textbf{} \>\> Felix Gutmann\\
\textbf{Student number:} 	\>\> 125604\\
\textbf{} 	\>\> 125584\\
\textbf{Program:} \>\> M.S. Data Science\\
\textbf{E-Mail:} \>\> domagoj.fizulic@barcelonagse.eu\\
\textbf{} \>\> felix.gutmann@barcelonagse.eu
\end{tabbing}
\vspace{1cm}


\pagebreak
% ----------------------------------------------------------------------------------------------------------
% Abstract
% ----------------------------------------------------------------------------------------------------------


\onehalfspacing




\pagebreak

% ----------------------------------------------------------------------------------------------------------
% Index
% ----------------------------------------------------------------------------------------------------------


\renewcommand{\cfttabpresnum}{Tab. }
\renewcommand{\cftfigpresnum}{Fig. }
\settowidth{\cfttabnumwidth}{Fig. 10\quad}
\settowidth{\cftfignumwidth}{Fig. 10\quad}

\titlespacing{\section}{0pt}{12pt plus 4pt minus 2pt}{2pt plus 2pt minus 2pt}
\singlespacing
\rhead{Table of contents}
\renewcommand{\contentsname}{I Table of Contents}
\phantomsection
\addcontentsline{toc}{section}{\texorpdfstring{I \hspace{0.35em}Table of Contents}{Table of Contents}}
\addtocounter{section}{1}


% ----------------------------------------------------------------------------------------------------------
% Table of contents
% ----------------------------------------------------------------------------------------------------------

\setcounter{page}{1}

\rhead{Table of Contents}

	\tableofcontents


\pagebreak
% ----------------------------------------------------------------------------------------------------------
% List of figures
% ----------------------------------------------------------------------------------------------------------

\rhead{List of Figures}

	\listoffigures
	
	
\pagebreak
% ----------------------------------------------------------------------------------------------------------
% List of tables
% ----------------------------------------------------------------------------------------------------------

\rhead{List of Tables}

	\listoftables
	

\pagebreak
%----------------------------------------------------------------------------------------------------------
% List of Listings
% ----------------------------------------------------------------------------------------------------------

\rhead{List of Algorithms}
\listofalgorithmes
\pagebreak

%----------------------------------------------------------------------------------------------------------
% List of Symbols
% ---------------------------------------------------------------------------------------------------------

\renewcommand{\arraystretch}{1.5}	
\section{List of mathematical symbols}
\rhead{List of mathematical Symbols}

\begin{tabular}{p{6cm}p{9cm}}
\textbf{Symbol} 		& 		\textbf{Meaning} \\
\midrule
\vspace{0.3cm} & \vspace{0.3cm} 			\\
$\mu$ & Mean of bandits \\
$\sigma$ & Standard deviation of bandits \\
$a$				  & Action 	\\
$Q(a)$	& Value function for action $a$\\
$t$ & Discrete time step t \\
$R(a)$ & Reward for action $a$ \\
$\epsilon$			& Probability of exploration in epsilon greedy \\
$\alpha$ & Learning rate \\ 
$\eta$ & Same parameter as $\alpha$ sometimes used in cognitive science\\
$\tau$ & Softmax or temperature parameter \\
$\theta$ & Inverse temperature parameter \\
$X$ & Random variable \\
$H(X) $ &  Entropy of a  discrete random variable $X$ \\ 
$d(\cdot,\cdot)$ & Distance Function \\
$S(\cdot,\cdot)$ & Similarity Function \\
$K(\cdot,\cdot)$ & Kernel function \\
$m(\cdot,\cdot)$ & Matching function \\ 
$\Theta(\cdot)$ & Heavy side step function \\
$ \mathbb{R}_0^+$ & Positive real numbers including zero \\
$\mathcal{X}$ & Data set \\ 
\textbf{W} & Weighted adjacency matrix \\
$d_i$ & Degree of node $i$ \\
\textbf{D} & Diagonal matrix of degrees \\
\textbf{L} & Graph laplacian \\
\end{tabular}

\pagebreak

%----------------------------------------------------------------------------------------------------------
% List of abbreviations
% ---------------------------------------------------------------------------------------------------------

\section{List of abbreviations}
\rhead{List of Abbreviations}

\begin{tabular}{p{6cm}p{9cm}}
\textbf{Abbreviations} & \textbf{Description} 										\\
\midrule
\vspace{0.3cm} & \vspace{0.3cm} 														\\ 
IGT & Iowa gambling task \\
RL & Reinforcement learning \\
DTW & Dynamic time warp \\
EDR & Edit distance on real sequences \\
CH & Choices \\
BBC & Blockwise bad choices \\
ENT & Cumulative entropy \\
BENT & Blockwise entropy \\
CC & Concatenated \\
NMI & Normalized mutual information score \\
ARI & Adjusted rand index \\
VM & V-Measure score 
\end{tabular}

\newpage

% ----------------------------------------------------------------------------------------------------------
% Prefix 2
% ----------------------------------------------------------------------------------------------------------

% Title spacing

%\titlespacing{\section}{1cm}{12pt plus 4pt minus 2pt}{-6pt plus 2pt minus 2pt}
%\titlespacing{\subsection}{0pt}{12pt plus 4pt minus 2pt}{-6pt plus 2pt minus 2pt}
%\titlespacing{\subsubsection}{0pt}{12pt plus 4pt minus 2pt}{-6pt plus 2pt minus 2pt}

% Header

\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\subsectionmark}[1]{}
\renewcommand{\subsubsectionmark}[1]{}
\lhead{Chapter \thesection}
\rhead{\rightmark}

\onehalfspacing

\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\theHsection}{\arabic{section}}
\setcounter{section}{0}
\pagenumbering{arabic}
\setcounter{page}{1}

%RGB Colour set

\definecolor{persblue}{rgb}{0.0862745,0.211765,0.360784}
\definecolor{persred}{rgb}{0.388235,0.145098,0.137255}
\definecolor{persgray}{rgb}{0.501961,0.501961,0.501961}
\definecolor{persgreen}{rgb}{0.054902,0.411765,0.352941}

%---------------------------------------------------------------------------------------------------------
% 1. Introduction
%---------------------------------------------------------------------------------------------------------

\section{Introduction and conceptual approach}

Decision-making is a cognitive process of selecting an option from a set of possible alternatives based on certain criteria \cite{Wang2007}. When analysing decision-making as a continuous process of interaction with the environment, learning becomes an important aspect. Learning is a complex procedure and can be described and estimated through different parameters. The learning procedure can be affected by different social and psychological conditions. Due to their cognitive ability people should show different learning behaviour.\\

One of the popular experiments for analysing decision behaviour is the Iowa Gambling Task. The decision making process is studied by monitoring peoples sequential choices in a controlled experiment environment.\\

In \textit{supervised learning} data are predicted by training a classifier based on examples. The identity of observations in the training sample is known. This is used to connect patterns in the data with corresponding labels of observations. In contrast to that in \textit{unsupervised learning} we don't know the ground truth. The objective is to discover natural clustering behaviour in the data itself and group objects into subsets, such that objects in those subsets are more closely related to each other \cite[page 9 et. seqq.]{Murphy2012} and \cite[page 501 et. seqq.]{hastie}. A vast class of clustering algorithms based on different approaches are proposed in the literature (e.g. hierarchical and optimization based clustering). This paper investigates, whether such unsupervised learning techniques can be used in the context of human decision making process to identify latent grouping. The decision making process is studied by monitoring peoples sequential choices in a controlled experiment environments over time. Thus, this paper operates in the intersection of machine learning and cognitive science. To our knowledge this particular setting has not been studied before.\\
We approach our research in the following way. We first set up a reinforcement learning based simulation framework to study theoretical boundaries of several clustering techniques and when they are applicable.
 Subsequently, we test our chosen methods on several  real experimental data sets. Corresponding to our simulation framework we first apply clustering algorithms to data from controlled n-armed bandit experiment.\footnote{A detailed introduction is given in section \ref{reinf}} A widely used approach to study human decision making process is the \textit{Iowa gambling task}, where participants try maximize rewards by choosing cards from different decks with different reward structures.\footnote{There are existing several slightly different versions of test. \cite{Steingroever2015} provides a data collection for from several sources giving a broad overview of different variations of the test.} Two decks have distributions with negative expectations while two have positive. However, each deck has its own variance and a set of profits.  Within this framework we analyse two different data sets. First we study decision making behaviour from people with different criminal profiles. Furthermore, we use another data set from cocaine abusers.\\


The report has the following structure. In section two we provide a short overview on related literature in the field. Section three is dedicated to the theoretical foundation and the simulation. We first provide knowledge of reinforcement and line out our experiment design in more detail. This section also includes an overview of applied algorithms, similarity and distance concepts. A mathematical formulation of the applied algorithms, similarity measures and cluster evaluation techniques can be found in the appendix.\footnote{Since our data are fairly small we will not discuss complexity of the algorithms.} Finally, we study their simulation performance and try to identify parameter settings, where they are applicable.\\ 
The rest of paper is address clustering our experimental data. We keep the scope of this paper tight and thus there some questions have to be left open. Thus we dedicated another section to discussing some possible extensions. We close this paper with a final summary of our results.

%---------------------------------------------------------------------------------------------------------
% 2. Literature
%---------------------------------------------------------------------------------------------------------

\section{Relevant Literature}

There exists a rich literature in cognitive science on identifying different behavioural groups. As mentioned in the introduction a commonly applied tool by a lot of studies is the Iowa Gambling Task experiment. Within that framework it has been shown that individuals with pre-frontal brain damage and decision-making defects continue to choose disadvantageously even after they learned the optimal strategy \cite{Bechara1997}.\\
A broad overview on other various results in the field can be found in \cite{Steingroever2013}. Several studies identify especially specific drug-user groups, e.g. cocaine addicts \cite{Stout2004}, chronic cannabis users \cite{Fridberg2010}, heavy alcohol users(heavy drinkers) \cite{Gullo2011}. Furthermore, extensive set of research is focused around particular mental disabilities, e.g. Asperger's disorder \cite{Johnson2006}, psychopathic tendencies \cite{Blair2001}, bipolar disorder \cite{Brambilla2012}, schizophrenia \cite{Martino2007} pathological gambling disorder \cite{Cavedini2002}, attention-deficit-hyperactivity disorder \cite{NiritAgay2010}. Most popular reinforcement learning models for identifying behavioural differences between different disorders are Expectancy Valence model \cite{Busemeyer2002} and Prospect Valence Learning model \cite{Ahn2008}.

%---------------------------------------------------------------------------------------------------------
% 3. Simmulation
%---------------------------------------------------------------------------------------------------------

\section{Theoretical Background and simulation experiments}

This section is dedicated to a detailed outline of our analysis approach and the results of our simulation experiments. The data we are analysing are gathered by observing peoples decisions over time. Hence, our data set are in the form $N \times M$ data set, where a row $N$ is the number of individuals and $M$ is the number of trials in the experiments. The data for each individual can be seen as a categorical time series. In terms of modeling there are two challenges. Some algorithms relying for example on euclidean distance while others operating on similarities. On one hand we introduce how repress those data and introduce related distance and similarity concepts for the algorithms. The section has a rather qualitative character. The appendix provides in more detail a mathematical background to applied algorithms, distance concepts and related clustering evaluation techniques. 

\subsection{Experiment design and problem formulation}

Figure \ref{fig:flow} depicts our simulation experiment design.\footnote{We implemented related coding for this project mainly in python. The code can be found on our  \href{https://github.com/FelixGSE/Master-Project}{github} repository.} The objective is to obtain a set of sequential choices for a given parameter setting of an artificial agent. We first generate a set of rewards by sampling $n-$vectors from a normal distribution ("\textit{multi arm bandits}"). The agent processes those rewards by sequentially choosing from those options. We repeat this procedure for several parameter settings for the agents and keep track of those choices which will define our data set.

 \begin{figure}[H]
	\includegraphics[width=\textwidth]{Pictures/flow01.png}
	\caption{Flowchart experiment design}
	\label{fig:flow}
\end{figure}

\subsection{Reinforcement Learning background and multi arm bandits}
\label{reinf}
Our simulation requires an artificial agent to produce desired data. The agent is relying \textit{Reinforcement Learning} (RL). Thus, we provide some necessary concepts of that field. The following definitions coming from \cite[chapter 1 and 2]{Sutton2012}. RL is a branch of \textit{machine learning} trying model the interaction of an artificial agents with its environment and the corresponding learning process. \\
In our particular setting the agent is confronted with the task of choosing sequentially from a set of $N$ possible choices. The agent doesn't have any knowledge about the system a priori. Therefore, it has to learn the nature of the system by continuously interacting with its environment while keeping track of the obtained information for each particular choice. Due to the lack of examples it has to explore different possible actions to identify the best action. Hence, it is useful to deviate from the current optimal strategy from time to time.\\ 
Each action in each step is associated with a given value based on the experience of the agent. This is modeled by defining a value function value function each action $a$. Denote the value function of an action $a$ as $Q(a)_t$. Hence the value function is defined average over the rewards for a given state. As mentioned it is necessary for the agent to explore its environment while simultaneously try to optimize its utility. Thus, a crucial task of the agent is to balance \textit{exploration} and \textit{exploitation} of the environment. There are two basic approaches to model this trade-off; An \textit{"Epsilon-Greedy"} action selection method and \textit{"Softmax"} selection method.\\
Within an epsilon greedy action selection strategy the next action is chosen based on the highest value function. However, to model exploration an random element is introduced to deviate from that greedy strategy with a certain probability (denoted by $\epsilon$). In general we can define the next action selected by the epsilon greedy strategy as:

\begin{flalign}
a_{t+1} = \begin{cases} 
\text{random action} & \text{, with probability } \epsilon \\
\arg \max_i Q(i)_t & \text{, with probability } 1-\epsilon
\end{cases} \nonumber
\end{flalign}

In the softmax action selection method each next action is sampled with a certain probability coming from a  \textit{Boltzmann Distribution)}. The probability for action a is computed by:

\begin{flalign}
P(a)_{t+1} = \frac{e^{\frac{Q_t(a)}{\tau}}}{\sum_{i}^{N} e^{\frac{Q_t(i)}{\tau}}}
\label{eq:softmax}
\end{flalign}

Using those probabilities for each action the next choice is sampled from this distribution. The softmax is essentially depending on the parameter $\tau$ parameter. It is controlling the how deterministic or random the agent is behaving. For increasing $\tau$ the numerator goes to one and the next action is therefore picked uniformly. For low values in $\tau$, actions with low value functions result in lower probabilities and hence in a greedy strategy. The parameter is sometimes called \textit{temparature}.\footnote{Another variation is to use the inverse of $\tau$ and denote it by $\theta$ (see, e.g. \cite{Stojic2015})}. 
After selecting an action the agent has to update its believes about it. The update rule is for the value function for an action is defined as:

\begin{flalign}
Q(a)_{t+1} = Q(a)_t + \alpha \left[ R(a)_t -  Q(a)_t	 \right], 
\label{eq:update}
\end{flalign}

where $\alpha$ is a is the non negative \textit{learning rate} defining how much the current action is affecting the believes and $R(a)_t$ is the reward of action a at step k. A challange might be how to initialize the value function. However, for convenience we set them to zero for all bandits. Those are the basic necessary ingredients to model artificial decision process. In the following we elaborate on further data processing.

\subsection{Data handling, unsupervised learning methods and similarities}
\label{algosim}
As mentioned we find two main challenges concerning the data. First, our data have a categorical nature. Furthermore, the learning process and corresponding behavioural changes also imposes changing dependence over time.\\
A well studied approach clustering such data types are \textit{hidden markov models}. For example, a decent study on that can be found in \cite{Pamminger2007}, \cite{Pamminger2009} and \cite{Pamminger2010a}. However, we focus our attention only on partition based clustering algorithms.\\
Some of the applied algorithms operate on distinct distance or similarity concepts. Given our data we  have to think carefully think about distance and similarity measures to respect the nature of our data and algorithms.\footnote{A formal definition of the distance and similarity can be found in the appendix} 
Furthermore, besides considering only raw choices we might try to re-express our data to discriminate them in different ways. 
One way is to apply \textit{Shannon's Entropy}  introduced by \cite{Shannon1948}. It is computed by using the empirical probability for each choices. For a discrete random variable  $X$ with probability $p$ the entropy is defined as \cite[page 32]{MacKay2005}:

\begin{flalign}
H(X) := -\sum_{i=1}^{N} p_i \log_2 p_i
\end{flalign}

Entropy gives measure on how random a random variable behaves.\footnote{From an applied perspective one get values for $X$ such that $p_i = 0$, we set $H(X) = 0$ for convenience  \cite[page 49]{Bishop2006}} Within the entropy framework we consider to types of entropies. For each time step we compute the entropy using choices done so far. We call this \textit{cumulative entropy}. Furthermore, we might want to observe more clearly how individuals adapt their behaviour over time. Considering an experiment with 100 trials we then compute the entropy for set of e.g. ten choices. We call this \textit{blockwise entropy}. Mapping choices to an entropy data set aims on discriminating individuals by their level randomness in their behaviour. \\
A second approach is based on the experimental setting. We know that within framework there are a set of choices, which are disadvantageous for the participant. Following e.g. \cite{Yechiam2008} or \cite{Ahn2008}, we then compute block wise the ratio of disadvantageous choices. We call this \textit{blockwise disadvantageous choice}. Finally we also consider a cumulative version of this ration, which we call \textit{cumulative disadvantageous choice}

Within our analysis we consider a broad selection of several clustering techniques and similarity concepts. Figure \ref{fig:cla} shows an overview of the algorithms and their corresponding distance/similarity requirements. A technical description for all of them is provided in the appendix. 

\begin{figure}[H]
	\includegraphics[width=\textwidth]{Pictures/DataClustering.jpeg}
	\caption{Input data, algorithms and proximity measures}
	\label{fig:cla}
\end{figure}

\subsection{Simulation setup and results}

We observed that that the clustering results of our simulation are fluctuating. Hence, to report stable results, the following numbers are computed as an average for each settings over 20 simulations. We also report the corresponding standard deviations to give an impression on the sensitivity of the clustering.\\
Our implementation is quite flexible. We can control both all parameters for each individual agent and their reward sets. We can control the number of trials for each agent and produce customized and multiple asymmetric cluster sizes. Furthermore, we can set prior values for each agent and each value function for the bandits. However, within our experiment we kept the settings simple. First we fix means and standard deviations for the bandits. Then first fix a value for alpha and run the simulations for different differences in tau. We repeat this procedure for fixed values of tau and vary alpha. Finally, we repeat this procedure with a different setting in the bandit means.\\
We are confronted with an enormous grid search over different parameters. Computing distances is very costly. As mentioned we have to run our simulations multiple times to get an impression of the variability. To keep the computation time to a reasonable we let the agents perform 100 trials and set the number of agents to 10 for each parameter setting. We ended up using solely the softmax function. The main reason for that decision is that we estimate those parameters later on for the experimental data.\\ 
The following table \ref{tab:simres}  shows a snippet of our simulation results. As described in the last section we have a broad range of similarity measures and algorithms. Therefore, we selected results in such a way, that we give an impression when clustering is working well. The results are selected in such a way, that they indicate the lower bound before clustering works perfectly. Naturally, we also found a lot of settings where clustering was not successful. A detailed overview on all numbers are given by tables \ref{tab:apstab1} to \ref{tab:apstab8} in the appendix.\footnote{To give an intuition about the numbers one can informally note that numbers below 0.200 - 0.300 do not show any really good clustering patterns.}
We find In general we find that our algorithms pick up the generated clusters when the differences in tau is sufficiently large. We tested this for different fixed values of $\alpha=\{0.1,0.5,1\}$. In general, we can say that if the difference in $\tau$ is larger than one, perfect clustering is possible and stable..\footnote{Standard deviation is equal to one} However, in some settings it also possible earlier.\footnote{See for example \ref{tab:apstab4} } \\
We repeated this procedure by fixing different values for $\tau$ and increase within each setting the values for $\alpha$. We are not able to achieve good clustering within that setting. In the summary table we listed an example, where we fix the difference in $\alpha$ to 0.9 and  still results are not very satisfying. Only if we fix the difference to 0.99 we are able to cluster within that setting.\\
Given the results we can conclude that we are able to discriminate individuals when their behaviour is significantly different with respect to randomness.\\
Concerning the data type the blockwise entropy seems to be overall the most successful one. In terms of algorithms we see that spectral clustering is in general outperforming the other algorithms. Furhtemore, the clustering results based on choices is constantly worse than the other data types.
\pagebreak

\begin{table}[!htbp] 
	\centering 
	\resizebox{\textwidth}{!}{\begin{tabular}{@{\extracolsep{0pt}} lllccccccccc} 
			\\[-1.8ex]\hline 
			\hline \\[-1.8ex] 
			& \textbf{Method} & \textbf{Similarity} & \textbf{Choices} & \textbf{Ratio disad. Choices}  & \textbf{Entropy}  & \textbf{Entropy Block} & \textbf{Concat} & \textbf{Normal. MI} & \textbf{Adj. Rand index} & \textbf{V-Measure} \\ 
			\hline 
			\textbf{Mu = \{0,2,4\}} &  &  &  &  &  &  &  &  &  &  \\ 
			\hline
			$\alpha$ = \{0.1\} & Spectral & RBF &  &  &  & x &  & 0.642 (0.200) & 0.653 (0.206) & 0.641 (0.200) \\ 
			$\tau$ = \{0.1, 0.7\} & Spectral & DTW &  &  & x &  &  & 0.580 (0.209) & 0.611 (0.205) & 0.580 (0.201) \\ 
			& K-Means & Euclidean &  &  &  &  & x & 0.382 (0.181) & 0.309 (0.237) & 0.381 (0.180) \\ 
			& Spectral & Levensthein & x &  &  &  &  & 0.350 (0.114) & 0.241 (0.163) & 0.347 (0.116) \\ 
			& Spectral & Euclidean &  & x &  &  &  & 0.306 (0.155) & 0.198 (0.179) & 0.303 (0.156) \\ 
			\hline
			\textbf{Average} &  &  &  &  &  &  &  & \textbf{0.367 (0.150)} &  \textbf{0.307 (0.195)} &  \textbf{0.364 (0.151)} \\ 
			\hline
			$\alpha$ = \{0.5\} & Spectral & DTW &  &  &  & x &  & 0.804 (0.212) & 0.804 (0.235) & 0.803 (0.213) \\ 
			$\tau$ = \{0.1, 1.0\} & Spectral & EDR &  &  & x &  &  & 0.652 (0.178) & 0.663 (0.203) & 0.652 (0.178) \\ 
			& Ward & Euclidean &  &  &  & x &  & 0.621 (0.208) & 0.596 (0.249) & 0.620 (0.209) \\ 
			& Spectral & Overlap & x &  &  &  &  & 0.599 (0.355) & 0.571 (0.406) & 0.597 (0.357) \\ 
			& K-Means & Euclidean &  &  &  & x &  & 0.562 (0.170) & 0.525 (0.220) & 0.562 (0.170) \\ 
			& Spectral & Levensthein & x &  &  &  &  & 0.368 (0.133) & 0.270 (0.169) & 0.366 (0.134) \\ 
			& K-Means & Euclidean & x &  &  &  &  & 0.275 (0.144) & 0.171 (0.147) & 0.272 (0.145) \\ 
			& K-Means & Euclidean &  &  &  &  & x & 0.399 (0.194)  & 0.338 (0.224) & 0.398 (0.194) \\ 
			& Spectral & Euclidean &  & x &  &  &  & 0.314 (0.109) & 0.197 (0.135) & 0.311 (0.110) \\ 
			\hline
			\textbf{Average} &  &  &  &  &  &  &  &   \textbf{0.408 (0.186)} &  \textbf{0.351 (0.217)} &  \textbf{0.406 (0.186)} \\ 
			\hline
			$\alpha$ = \{1.0\} & Spectral  & DTW &  &  &  & x &  & 0.792 (0.174) & 0.808 (0.170) & 0.792 (0.174) \\ 
			$\tau$ = \{0.1, 1.0\} &  Spectral  & EDR &  & x &  &  &  & 0.280 (0.198) & 0.201 (0.215) & 0.278 (0.198) \\ 
			& Spectral  & Levensthein & x &  &  &  &  & 0.276 (0.101) & 0.154 (0.113) & 0.272 (0.111) \\ 
			& Spectral  & EDR &  &  & x &  &  & 0.236 (0.196) & 0.171 (0.184) & 0.235 (0.195) \\ 
			& K-Means & Euclidean  &  &  &  &  & x & 0.099 (0.105) & 0.052 (0.097) & 0.098 (0.104) \\ 
			\hline
			\textbf{Average} &  &  &  &  &  &  &  &   \textbf{0.266 (0.203)} &  \textbf{0.204 (0.225)} &  \textbf{0.263 (0.204)} \\ 
			\hline
			\textbf{Mu = \{0,1,2\}} &  &  &  &  &  &  &  &  &  &  \\ 
			\hline
			$\alpha$ =  \{0.1\} & Spectral & DTW &  &  &  & x &  & 0.824 (0.166) & 0.837 (0.170) & 0.824 (0.166) \\ 
			$\tau$ = 	\{0.1, 0.5\} & Spectral & DTW &  &  & x &  &  & 0.636 (0.213) & 0.661 (0.205) & 0.636 (0.213) \\ 
			& K-Means & Euclidean &  &  &  &  & x & 0.532 (0.182) & 0.543 (0.206) & 0.531 (0.182) \\ 
			& Spectral & Levensthein & x &  &  &  &  & 0.411 (0.125) & 0.333 (0.160) & 0.410 (0.127) \\ 
			& Spectral & Euclidean &  & x &  &  &  & 0.242 (0.163) & 0.132 (0.187) & 0.238 (0.165) \\ 
			\hline
			\textbf{Average} &  &  &  &  &  &  &  &  \textbf{0.394 (0.210)} &  \textbf{0.338 (0.257)} &  \textbf{0.391 (0.212)} \\ 
			\hline
			$\alpha$ = \{0.5\} & Spectral & DTW &  &  &  & x &  & 0.769 (0.179) & 0.783 (0.185) & 0.769 (0.179) \\ 
			$\tau$ =  \{0.1, 0.5\} & Spectral & EDR &  & x &  &  &  & 0.335 (0.172) & 0.252 (0.191) & 0.333 (0.174) \\ 
			& Spectral & Levensthein & x &  &  &  &  & 0.324 (0.116) & 0.213 (0.128) & 0.322 (0.117) \\ 
			& Spectral & EDR &  &  & x &  &  & 0.221 (0.182) & 0.149 (0.161) & 0.221 (0.181) \\ 
			& K-Means & Euclidean &  &  &  &  & x & 0.137 (0.137) & 0.110 (0.152) & 0.136 (0.137) \\ 
			\hline
			\textbf{Average}  &  &  &  &  &  &  &  &  \textbf{0.248 (0.202)} &  \textbf{0.196 (0.212)} &  \textbf{0.246 (0.203)} \\ 
			\hline
			$\alpha$ = \{1.0\} & Spectral & DTW &  &  &  & x &  & 0.618 (0.192) & 0.627 (0.201) & 0.617 (0.192) \\ 
			$\tau$ = \{0.1, 0.7\} & Spectral & Levensthein & x &  &  &  &  & 0.252 (0.137) & 0.163 (0.147) & 0.249 (0.137) \\ 
			& Spectral & RBF &  &  & x &  &  & 0.215 (0.158) & 0.207 (0.188) & 0.215 (0.157) \\ 
			& Spectral & DTW &  & x &  &  &  & 0.205 (0.218) & 0.192 (0.247) & 0.205 (0.218) \\ 
			& K-Means & Euclidean &  &  &  &  & x & 0.185 (0.205) & 0.165 (0.225) & 0.185 (0.205) \\ 
			\hline
			\textbf{Average} &  &  &  &  &  &  &  &  \textbf{0.248 (0.183)} &  \textbf{0.220 (0.213)} &  \textbf{0.246 (0.184)} \\ 
			\hline
			\textbf{Mu = \{0,2,4\}}\tnote{*} &  &  &  &  &  &  &  &  &  &  \\ 
			\hline
			$\tau$ = \{1.0\}  & Spectral & DTW &  &  &  & x &  & 0.300 (0.232) & 0.273 (0.254) & 0.299 (0.232) \\ 
			$\alpha$ = \{0.1,0.9\} & Spectral & Cosine &  & x &  &  &  & 0.211 (0.158) & 0.159 (0.160) & 0.210 (0.158) \\ 
			& Spectral & Cosine &  &  & x &  &  & 0.191 (0.117) & 0.137 (0.127) & 0.189 (0.118) \\ 
			& Spectral & Cosine & x &  &  &  &  & 0.164 (0.110) & 0.115 (0.109) & 0.162 (0.109) \\ 
			& K-Means & Euclidean &  &  &  &  & x & 0.138 (0.153) & 0.091 (0.176) & 0.137 (0.153) \\ 
			\hline
			\textbf{Average}  &  &  &  &  &  &  &  &  \textbf{0.145 (0.067)} &  \textbf{0.101 (0.075)} &  \textbf{0.143 (0.067)} \\ 
			\hline \\[-1.8ex] 
		\end{tabular}}
		\begin{tcolorbox}[arc=0mm,title=Notes,boxrule=0.2mm,colbacktitle=white,coltitle=black,colback=white,top=0mm,bottom=0.1mm]
			\begin{tablenotes}
				\item The tables shows something
				\item[*] Selected result for fixed $\tau$ and varying and different $\alpha$
			\end{tablenotes}
		\end{tcolorbox}
		\caption{Selected simulation results}
		\label{tab:simres} 
	\end{table}


%---------------------------------------------------------------------------------------------------------
% 4. Data
%---------------------------------------------------------------------------------------------------------

\section{Data Analysis}

Our simulation results suggested that we can cluster the decision behaviour under some given constraints. In the following we apply our methods to three different real experiment data. The first data comes  As mentioned in the introduction the Iowa gambling task is popular way to monitor decision and learning process of individuals. s
In the simulation setting we initially define participants with a certain set of parameters. There exist techniques to estimate those parameters from actual data. 

\subsection{Multi-arm bandit experiment data}

The first data set is related to \cite{Stojic2015}. The data are gathered in a 20-arm bandits online experiment, in which users were compensated with small amount of money in exchange. Four different distributional settings were given to different people. In total the data sets consists of 429 participants divided in 199 female and 229 male participants.\footnote{One did not wish to answer} The average age 33.04 with standard deviation 11.75. Furthermore, the participants overall have a stronger higher education background. 261 participants have college degree, 39 a graduate degree and PhD respectively. 127 have a high school degree and 2 declined to answer.\\
We tried to identify different clusterings according to those demographics within those four sub experiments. Our results doesn't show worth mentioning clustering across demographics.\\
We try to discover clustering in the data. Within our simulation we set parameters, which classify individual subjects. Using the experimental data we come from the other way. The reinforcement learning model is quite closely related to the expectancy valence model from cognitive science. Referring to equation \eqref{eq:softmax} and equation \eqref{eq:update} we can try to recover the parameters by optimising those function based on the observed choices. 

\begin{figure}[H]
	\centering
	\small
	\hspace*{-0.7in}
	\begin{tabular}{cc}
		\input{Pictures/e1c2.tex} & \input{Pictures/e1c3.tex} \\
		(\textbf{a}) Spectral - RBF - blockwise entropy & (\textbf{b}) Spectral - DTW - blockwise entropy \\
			\input{Pictures/e2c2.tex} & \input{Pictures/e2c3.tex} \\
			(\textbf{c}) Ward clustering - blockwise disadvantageous choices & (\textbf{d}) Spectral - cosine - blockwise disadvantageous choices 
	\end{tabular} \quad
	\caption{Clustering on choices vs. model parameter estimation (top sub figures: experiment 1, high noise, bottom: experiment 1, low noise)}
	\label{fig:cluse1c2}
\end{figure}




\subsection{Prison data}

We were provided with experimental data from \cite{Yechiam2008}. Again we try to apply our methods to cluster different groups in the data. The participants had to perform a modified version of the Iowa gambling task, where reward structure of the decks are changing over time.\\
In particular we have data of 96 individuals with different criminal profile. Within this data we don't have a control group. Given that participants performed a different version of the test we could add a control data from publicly available data sets (see \cite{Steingroever2015}).\\
Table \ref{tab:tabps} gives a broad summary of some demographics of the participants. The samples for each groups are not balanced. 

\begin{table}[H]
	\scriptsize
	 \centering 
	 		\caption{Summary prison data (means with standard deviation in parenthesis)} 
	\resizebox{\textwidth}{!}{\begin{tabular}{ llrcccc} 
		\toprule 
		\textbf{ID} & \textbf{Criminal profile} & \textbf{Count} & \textbf{Age} & \textbf{TABE Score} & \textbf{Education} & \textbf{Beta IQ} \\ 
		\hline
	1&Theft/Burglary & $22$ & $25.36$  (7.03) & $11.09$ (1.29) & $7.38$ (3.34 )& $92.91$ (14.37) \\ 
	2&Robbery & $6$ & $24.17$ (9.83) & $11.00$ (0.63) & $9.22$ (3.30) & $96.50$ (7.58) \\          
	3&Sex & $17$ & $33.41$ (13.59) & $10.97$ (1.47) & $9.15$ (2.98) & $99.65$ (11.74) \\       
	4&Drug & $22$ & $30.91$ (10.11)  & $11.64$ (1.85)& $9.06$   (2.70)  & $100.36$ (12.92) \\     
	5&OWI & $4$ & $38.75$ (7.27) & $10.88$ (1.93) & $7.12$ (1.17)& $94.25$ (10.40) \\        
	6&Assault & $10$ & $27.20$ (8.77) & $12.30$ (2.41)& $7.62$ (2.28) & $94.50$ (11.29) \\       
	7&Escape/ Failure To Appear & $4$ & $2.008$ (5.60)& $11.00$ (1.35)& $7.78$  (3.21)& $96.50$  (14.18)\\     
	8&Vandalism & $1$ & $18.00$ (NA)& $11.00$ (NA)& $9.40$ (NA)& $90.00$ (NA)\\ 
	9&Forgery & $7$ & $34.57$ (13.14)& $10.93$ (5.15) & $9.83$ (3.82)& $100.71$ (11.01)\\       
	10&Probabiton & $1$ & $38.00$ (NA)& $12.00$ (NA)& $6.30$ (NA)& $92.00$ (NA)\\ 
	11 &Other & $2$ & $35.00$ (9.90)& $11.50$ (0.00)& $9.20$ (4.67)& $95.00$ (5.66)\\       
		\bottomrule 
	\end{tabular} }
		\label{tab:tabps} 
\end{table} 

In figure \ref{fig:ent} we show display some of the behavioural data. Given the number of participants we averaged the by criminal profile to see if we can find differences in across groups. We excluded "OWI", "escape", "vandalism", "probation" and "others" from this figures since the number of observations is fairly small. This also gives a more clear insight in the data.\\
Figure \ref{fig:ent} (a) and (b) show the blockwise ratio of disadvantageous choices and the development of cumulative disadvantageous choices. A clear separation can not be observed from those plots. From figure (b) we can at most see a slight decreasing trend for forgery, sex robbery and theft, while drug and stay slightly higher.\footnote{\cite{stout2004} found a constant disadvantageous behaviour for cocaine abusers (see next section)}\\
Figure \ref{fig:ent} (d) shows the average cumulative entropy averaged across groups. We observe a random behaviour independent from group affiliation. Notably, people with assault/murder profile are show constantly the highest value. We can observe the same from the blockwise entropy from figure (c) we could observe the same behaviour.

{\renewcommand{\arraystretch}{0.4}%
\begin{figure}[H]
	\centering
	\small
	\hspace*{-0.7in}
	\begin{tabular}{cc}
	\input{Pictures/disad.tex} & \input{Pictures/cumbad.tex} \\
	(\textbf{a}) Block wise picks from disadvantageous deck & (\textbf{b}) Cumulative picks from disadvantageous deck  \\
	\input{Pictures/entbl.tex} & \input{Pictures/avgent.tex} \\
	(\textbf{c}) Block wise entropy  & (\textbf{d}) Cumulative entropy 
	\end{tabular} \quad
	\caption{Disadvantageous behaviour and entropy averaged by criminal profile }
	\label{fig:ent}
\end{figure}

\cite{Yechiam2008} found three clusters by using the attention to recent outcomes(ARO) and attention to gains(AG) parameters from the Expectancy Valance model. The most distinct group were the Robbery convicts with the only negative attention to gains mean at -0.36 and the highest attention to recent outcomes mean 0.57. The second cluster is made of assault and murder convicts with ARO of 0.26 and AG of 0.1. The third cluster is formed of all the remaining prisoner groups with ARO means between 0 and -0.1 and AG between 0.1 and 0.2.\\
Based on those findings and what we observed in figure \ref{fig:ent} we decided to cluster convicted assault-murder and robbery individuals against the other criminal groups. The following table depicts selected resulting clustering performance.\footnote{The remaining results can be found in table  \ref{tab:prison1} and \ref{tab:prison2} in the appendix} We find the most accurate clustering among all between assault/murders and forgery, both on entropy and and cumulative bad choices, which is suggested by the descriptive findings from figure \ref{fig:ent}. The listed algorithms performed remarkably better than the average performance. Furthermore, we managed to find some weaker clustering results between robbery. In contrast to \cite{Yechiam2008} we manged to isolate rather assault/murder criminals from the rest than robbery.

\begin{table}[!htbp] \centering 
	\caption{} 
	\label{} 
	\footnotesize
	\resizebox{\textwidth}{!}{\begin{tabular}{@{\extracolsep{0pt}} lllccccccccc} 
\toprule 
		\textbf{Groups} & \textbf{Method} & \textbf{Similarity} & \textbf{C} & \textbf{CBC} & \textbf{BBC} &\textbf{ E }& \textbf{EB} & \textbf{CC} & \textbf{NMI} & \textbf{ARI} & \textbf{VM} \\ 
		\hline 
				2 vs. 9 & Spectral & EDR &  &  &  & x &  &  & 0.382 (0.000) & 0.235 (0.000) & 0.382 (0.000)\\ 
				& Spectral & EDR &  &  & x &  &  &  & 0.232 (0.000) & 0.226 (0.000) & 0.232 (0.000)\\ 
				& Spectral & Cosine &  &  &  &  & x &  & 0.197 (0.000) & 0.008 (0.000)& 0.191 (0.000)\\ 
				& Spectral & RBF &  & x &  &  &  &  & 0.134 (0.000)& 0.075 (0.000)& 0.134 (0.000) \\ 
				& K-Means & Euclidean &  &  &  &  &  & x & 0.116 (0.000) & - & 0.105 (0.000) \\ 
				& K-Means & Euclidean & x &  &  &  &  &  & 0.111 (0.022) & - & 0.100 (0.020) \\
						\hline
						\textbf{Average} &  &  &  &  &  &  &  &  & \textbf{0.096 (0.074)} & \textbf{-} & \textbf{0.093 (0.073)} \\ 
				\hline 
		6 vs. 9 & Ward  & Euclidean &  &  &  &  & x &  & 0.561 (0.000) & 0.560 (0,000) & 0.560 (0,000) \\ 
		& Spectral & DTW &  &  &  & x &  &  & 0.435 (0.000) & 0.387 (0.000) & 0.432 (0.000) \\ 
		& Spectral & DTW &  & x &  &  &  &  & 0.435 (0.000) & 0.387 (0.000) & 0.432 (0.000) \\ 
		& Spectral & EDR &  &  & x &  &  &  & 0.333 (0.000) & 0.381 (0.000) & 0.333 (0.000) \\ 
		& Spectral & Overlap & x &  &  &  &  &  & 0.111 (0.000) & 0.118 (0.000) & 0.111 (0.000) \\ 
		& K-Means & Euclidean &  &  &  &  &  & x & 0.106 (0.066) & 0.033 (0.054) & 0.094 (0.059) \\
		\hline
		 \textbf{Average}& &  &  &  &  &  &  &  & \textbf{0.249 (0.167)} & \textbf{0.202 (0.189)} & \textbf{0.244 (0.169)} \\ 
		\hline \\[-1.8ex] 
	\end{tabular} }
\end{table} 


\subsection{Cocaine Abusers data}

Finally we study data from several cocaine abusers. There are 12 individuals performing the IGT. The control group consist out of 14 participants. Candidates among the drug abusers were selected as active users with additional drug abusing past, but without any known additional mental illness \cite{Stout2004}. Table \ref{tab:cocs} gives a summary of demographic profile.

\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1}
\begin{table}[H]
	\centering 
		\caption{Demographic summary of cocaine abusers (means with standard deviations in parenthesis)}
	\begin{tabular}{lcc}
		\toprule
		\textbf{Demographic indicator} & \textbf{Drug abusers} & \textbf{Control Group} \\
		\hline
		Share of men &  79\% & 100\%\\
		Age & 36.90 (10.30) & 30.00 (6.10) \\
		Estimated IQ & 105.00 (7.62) & 93.70 (10.30) \\
		\bottomrule
	\end{tabular}
	\label{tab:cocs}
\end{table}

Within the gambling task they found that cocaine abusers choose persistently cards from disadvantageous decks. The effect is still present after controlling for the low IQ score.\\
We cluster cocaine abusers against the control group. The following table \ref{tab:cocainresults} depicts results of the clustering. Again we averaged over 20 simulations to report overall clustering performance. However as depicted the data set is friarly small and results are quite stable.\\
Again our best clustering is achieved using block wise entropy, besides the listed K-Means algorithm, spectral clustering based on both cosine similarity and with rbf kernel achieved the same results. However, the results for this data set are rather low and we could not find  good clustering between the control group and cocaine abusers. This also is indicated by the average over all applied methods. 

\begin{threeparttable}[H] 
	\centering
	\label{} 
	\scriptsize
		\resizebox{\textwidth}{!}{\begin{tabular}{ llccc cccccc} 
		\toprule
			\textbf{Method} & \textbf{Similarity} & \textbf{C}\tnote{0} & \textbf{CBC}\tnote{1} & \textbf{BBC}\tnote{2} & \textbf{E}\tnote{3} & \textbf{EB}\tnote{4} & \textbf{CC}\tnote{5} & \textbf{NMI}\tnote{6} & \textbf{AR}I\tnote{6} & \textbf{VM}\tnote{7} \\ 
		\hline 
		K-Means \tnote{*} & Euclidean &  &  &  &  & x &  & 0.270 (0.000) & 0.262 (0.000) & 0.270 (0.000) \\ 
		Ward & Euclidean &  & x &  &  &  &  & 0.270 (0.000) & 0.262 (0.000) & 0.270 (0.000) \\ 
		K-Means & Euclidean &  &  &  &  &  & x & 0.209 (0.030) & 0.171 (0.043) & 0.208 (0.030) \\ 
		Spectral & Levenstein & x &  &  &  &  &  & 0.178 (0.000) & 0.181 (0.000) & 0.178 (0.000) \\ 
		Spectral & Cosine &  &  & x &  &  &  & 0.171 (0.000) & 0.117 (0.000) & 0.171 (0.000) \\ 
		Spectral & DTW &  &  &  & x &  &  & 0.042 (0,000) & 0.014 (0.000) & 0.042 (0.000) \\ 
		\hline
		\textbf{Average}\tnote{$\dagger$}  &  &  &  &  &  &  & & \textbf{0.139 (0.002)} & \textbf{0.104 (0.003)} & \textbf{0.138 (0.002)} \\ 
		\hline 
	\end{tabular} }
	\begin{tcolorbox}[arc=0mm,title=Notes,boxrule=0.2mm,colbacktitle=white,coltitle=black,colback=white,top=0mm,bottom=0.1mm]
	\begin{tablenotes}
	       	\item[0] Clustering based on choices participant did 
	       	\item[1] Clustering based on cumulative disadvantageous choice of participants
	        \item[2] Clustering based on block wise disadvantageous choice. Block size = 10
	        \item[3] Clustering based on cumulative entropy
	        \item[4] Clustering based on block wise entropy. Block size = 10
	        \item[5] Clustering based on entropy and choices concatenated for each participant
	        \item[6] Normalised mutual infrormation score (description see appendix \ref{sec:ce})
	        \item[7] Adjusted rand index (description see appendix \ref{sec:ce})
	        \item[8] V-Measure (description see appendix \ref{sec:ce})
	        \item[*] Spectral Clustering on block wise entropy with RBF kernel and cosine similarity produced the same results
	        \item[$\dagger$] Average over all algorithms including the ones displayed in the table
	\end{tablenotes}
	\end{tcolorbox}
	\caption{Clustering results for cocaine abusers vs. control group} 
	\label{tab:cocainresults}
 \end{threeparttable} 

\section{Discussion of results and possible extensions }

Our analysis so far showed that people are not separate themselves. In general we assume that healthy participants and those with assumed decision making deficits show significantly different behaviour. However, we observe that their behaviour seem to be quite similar given our data and applied mappings. Furthermore , in most of the applied unsupervised techniques we as analysts have to to set the number of clusters we assume to be in the data (so in our case two for control group and patients with habits). We apply another algorithm called affinity propagation, which identifies the number of clusters itself (algorithm formulation see appendix). In general we find that the algorithm is assign , which suggest that there more natural clusters in the data than the  one we assume due to their status labeled as healthy and ill.

%---------------------------------------------------------------------------------------------------------
% 5. Data
%---------------------------------------------------------------------------------------------------------

\section{Conclusion}

Maybe process also reward data 


\section{Acknowledgment}




\pagebreak
% ----------------------------------------------------------------------------------------------------------
% Literature
% ----------------------------------------------------------------------------------------------------------

\renewcommand\refname{List of Literature}

\bibliographystyle{apalike}

\bibliography{unsupervised}


\pagebreak

%----------------------------------------------------------------------------------------------------------
% Appendix
% ---------------------------------------------------------------------------------------------------------
\lhead{Appendix \thesection}
\rhead{}
\pagenumbering{Alph}
\setcounter{page}{1}

\begin{appendix}
	
\section*{Appendix}
\phantomsection
\addcontentsline{toc}{section}{Appendix}
\addtocontents{toc}{\vspace{-0.5em}}

\section{Metrics and Similarities}

This part of the appendix formally defines metrics and similarities and dissimilarities (proximity) used in this paper. We first define some basic general concepts followed by a description of the applied distance and similarity concepts. 

\subsection{Distances vs. Similarities}

Let $\mathcal{X}$ be a dataset and let $\boldsymbol{x_i},\boldsymbol{x_j}$ be two datapoints, such that $\boldsymbol{x_i},\boldsymbol{x_j} \in \mathcal{X}$. 

A distance function assign for pairs a points a non negative real number as distance. $d:\mathcal{X}\times \mathcal{X} \mapsto \mathbb{R}_0^+$. Formally if the following properties are additionally staisfied the distance is also metric \cite[page 28]{Shirali06a}.

\begin{enumerate}
	\setlength{\itemsep}{-5pt}
	\item $d(\boldsymbol{x_i},\boldsymbol{x_j}) \ge 0$
	\item $d(\boldsymbol{x_i},\boldsymbol{x_i}) \ge 0$
	\item $d(\boldsymbol{x_i},\boldsymbol{x_j}) = d(\boldsymbol{x_j},\boldsymbol{x_i}) $
	\item $d(\boldsymbol{x_i},\boldsymbol{x_j}) \le d(\boldsymbol{x_i},\boldsymbol{x_j})+ d(\boldsymbol{x_j},\boldsymbol{x_i}) $
\end{enumerate}

A distance can be seen as a measure for dissimilarity of two points \cite[page 35]{Everitt2009}. Besides distance some algorithms operate on a \textit{similarity} matrix. Formally a similarity is a function  $ S : \mathcal{X} \times \mathcal{X} \mapsto [0,1] $. Also for similarity we can define the following properties \cite[page 3]{Fratev1979}:

\begin{enumerate}
	\setlength{\itemsep}{-5pt}
	\item $0 \le S(\boldsymbol{x_i},\boldsymbol{x_j}) \le 1, \text{for } i \neq j$
	\item $S(\boldsymbol{x_i},\boldsymbol{x_i}) = 1$
	\item $S(\boldsymbol{x_i},\boldsymbol{x_j}) = S(\boldsymbol{x_j},\boldsymbol{x_i})$
\end{enumerate}

Once we have computed distance or a similarity we can compute for two data points we can use this information to transform it to a similarity or the distance vice versa \cite[page 4]{Boriah2008}:

\begin{flalign}
S(\boldsymbol{x_i},\boldsymbol{x_j}) = \frac{1}{1+d(\boldsymbol{x_i},\boldsymbol{x_j}) } \hspace{0.5cm} \Leftrightarrow \hspace{0.5cm} d(\boldsymbol{x_i},\boldsymbol{x_j}) =  \frac{1}{S(\boldsymbol{x_i},\boldsymbol{x_j})} -1 
\label{eq:sim}
\end{flalign}

\subsection{General Similarity measures}

The default similarity measures used by many machine learning libraries (e.g. python sci-kit) is the Gaussian kernel of RBF-Kernel. This kernel function can be also be seen as a similarity. For two point is is defined as \cite[page 480]{Murphy2012}

\begin{flalign}
K_{RBF}(\boldsymbol{x_i},\boldsymbol{x_j}) = \exp{\left(-\frac{|| \textbf{x} - \textbf{y} ||}{2 \sigma^2}\right) }
\end{flalign}

Another common similarity measure is cosine similarity. It is expressing the angle between two vectors and is defined as [ibidem, page 480]:

\begin{flalign}
S_{cos}(\boldsymbol{x_i},\boldsymbol{x_j}) = \frac{\textbf{x}^T\textbf{x}}{||\textbf{x}|| ||\textbf{y}||}
\end{flalign}

\subsection{Similarity measures for categorical data}

Referring to equation \ref{eq:ets} we define a simple measure for the categorical aspect of the data. Note that [SOURCE] defines the simplest measure for categorical data. However, since the probability that two people behave exactly the same in our context is arguably zero we modify this concept slightly. So we relax that and define the overlap similarity just as the count of overlapping instances. This serves as a benchmark similarity for categorical data.

\begin{flalign}
d_O (\boldsymbol{x},\boldsymbol{y}) :=  \sum_{i=1}^{N} \mathbb{1}_{(x_i = y_i)}
\end{flalign}

Furthermore, Levenstein distance (or edit distance) is a basic way to measure similarity between categorical sequences \cite[page 1]{Richter} or \cite[page 2]{Gabadinho2009}, however some authors stating that it might perform poorly in their task \cite[page 3]{Ren2011} or a poor measure at all \cite[page 5]{Morzy}

It is relying on solving a dynamic programming problem. Define $D_{0,j} = j$ and $D_{i,0} = i$:
\begin{flalign}
D_{\textbf{x},\textbf{y}}(i,j) = \min \begin{cases} D_{i-1,j} + 1\\
D_{i,j-1} + 1 \\
D_{i-1,j-1} + \mathbb{1}_{(x_i = y_i)}
\end{cases}
\end{flalign}

\subsection{Similarity measures for time series data}

We use three different similarity measures for time series. E.g. \cite{Wang2013} or \cite{Serr2014} provide an overview and empirical evaluation on common similarity measures for time series. The following definitions are taken from the latter. Empirical research suggest that simple euclidean distance for time series performs quite well and is hard to beat. Hence, the first distance measure for time series is simply euclidean distance between two time series. They might be converted to similarity based on equation \ref{eq:sim}. Let $\boldsymbol{x},\boldsymbol{y}$ be two time series over N-periods. Then the $L_2$ distance between two time series is define as: 

\begin{flalign}
d_E (\boldsymbol{x},\boldsymbol{y}) :=  \sqrt{\left( \sum_{i=1}^{N} \left( x_i - y_i \right)^2 \right)}
\label{eq:ets}
\end{flalign}
\pagebreak

Dynamic time warp (DTW) is probably one of the most successful proximity measures for time series. While in euclidian distance we compare all points horizontally in DTW we take also other points into account. It is using dynamic programing to solve it. 


Finally we considered the edit distance on real sequences (EDR). It is basically an real valued version to the Levensthein distance. It also relies on solving a dynamic programing problem. For $i = 1,\dots,M$ and $j=1,\dots,N$ we have to compute.\footnote{The time series can have different length. However, in our case $N=M$}

\begin{flalign}
D_{i,j} = \begin{cases} 
D_{i-1,j-1} & \text{,if } m(x_i,y_j) = 1 \\
1 + \min(D_{i,j-1},D_{i-1,j},D_{i-1,j-1})& \text{, if } m(x_i,y_j) = 0
\end{cases}
\end{flalign}

where, $m(\cdot,\cdots)$ is the matching function. For $x_i and y_j$ it is defined as: 

\begin{flalign}
m(xi,yj)= \Theta(\epsilon - f(x_i,y_j))
\end{flalign}

where $\epsilon$ is scalar, such that $\mathbb{R}^+_0$. $\Theta(\cdot)$ denotes the Heaviside step function and is defined as $\Theta(z) = 1$ if $z\ge0$.
 
\pagebreak
\section{Algorithms}

\subsection{K-Means Clustering}

The following algorithm \ref{alg:kmean} describes the K-means clustering algorithm. The algorithm comes from \cite[page 354 et. seqq.]{Murphy2012}

\IncMargin{1em}
\begin{algorithm}
	\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
	\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
	\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
	\Input{$\boldsymbol{m}_k$}
	\BlankLine
	\Repeat{converged}{
		\begin{itemize}
			\item 	Assign each data point to its closest cluster center: $z_i = \arg \min_k || \boldsymbol{x}_i - \boldsymbol{\mu}_k ||^2_2;$ 
			\item 	Update each cluster center by computing the mean of all points assigned to it: $\boldsymbol{\mu}_k = \frac{1}{N_k}\sum_{i:z_i=k}\boldsymbol{x_i};$
		\end{itemize}}
	\caption{K-Means clustering}\label{algo_disjdecomp}
	\label{alg:kmean}
\end{algorithm}\DecMargin{1em}
\pagebreak

\subsection{Hierarchical Clustering - Agglomerative}

Algorithm \ref{alg:agglc} \cite[page 895 et. seqq.]{Murphy2012}

\IncMargin{1em}
\begin{algorithm}
	\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
	\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
	\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
	\Input{initialize clusters as singletons:}
	\Output{Importance values for each node $v$}
	\BlankLine
	\For{$ i \leftarrow 1 \textbf{\ to } n$}{
	$C_i \leftarrow \{i\}$;
	}
	\text{Initialize set of clusters for merging:} \\
	    $S \leftarrow \{1, \dots, n\};$ \\
	\Repeat{convergence}{
		\text{Pick 2 most similar clusters to merge:}; \\ 
		$(j,k) \leftarrow \arg \min_{j,k \in S}d_{j,k};$ \\
		\text{Create new cluster} $C_\ell \leftarrow C_j \cup C_k$
		\text{Mark j and k as unavailable:} $S  \leftarrow S \setminus \{j,k\};$
		\If{$C_l \neq \{1,\dots,n\}$}{Mark $\ell$ as available: $S \leftarrow S \cup \{\ell\}$ ;}
		\For{$i \in S$}{Update dissimilarity matrix $d(i, \ell)$;}
		no more clusters are available for merging;}
		
	\caption{Agglomerative Clustering}\label{algo_disjdecomp}
	\label{alg:agglc}
\end{algorithm}\DecMargin{1em}

\pagebreak
\subsection{Spectral Clustering}

For the spectral clustering algorithm we formally introduce some graph notation. If not stated otherwise the following derivation follows \cite{Luxburg2007}. In the following we consider a weighted and simple undirected graph. 
\begin{flalign}
G &= \{V,E\} \\
V & = \{ v_1,\dots,v_n \} \\
E & = \{e_1,\dots,e_n\} 
\intertext{Furthermore let the graph has a weighted and symetric ($|V| \times |V|$) adjacency matrix, such that:}
\boldsymbol{W} &= \begin{cases} 
w_{i,j} & \text{,if } v_iv_j \in E \\
0 & \text{otherwise }
\end{cases}
\intertext{The \textit{degree} of a node is defined as the sum of edge weights of connected nodes. Formmally we denote the degree of node $i$ as:}
d_i &:= \sum_{j=1}^{n} w_{ij} = \sum_{i=1}^{n} w_{ij}
\intertext{Using the last expression we define matrix $\boldsymbol{D}$ as the diagonal matrix of the degress}
\boldsymbol{D} &:= diag(\boldsymbol{d})
\intertext{The algorithm works on the \textit{Laplacian} matrix defined by:}
\boldsymbol{L} &:= \boldsymbol{D} - \boldsymbol{W} 
\intertext{Former versions of the algorithm are applied on the graph laplcian. However, there were proposed newer versions using the so called \textit{normalized laplacian}. Since also the python version is using this package we will focus on this version of the algorithm. Following that the normalized graph laplacian is defined as:}
\boldsymbol{L}_{norm} &:= \boldsymbol{D}^{1/2} \boldsymbol{L} \boldsymbol{D}^{1/2}  = \boldsymbol{I} - \boldsymbol{D}^{1/2} \boldsymbol{W} \boldsymbol{D}^{1/2}
\end{flalign}

IncMargin{1em}
\begin{algorithm}
	\SetKwData{Left}{left}\SetKwData{This}{this}\SetKwData{Up}{up}
	\SetKwFunction{Union}{Union}\SetKwFunction{FindCompress}{FindCompress}
	\SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
	\Input{Similarity matrix $S \in \mathbb{R}^{n \times n}$ and number of clusters $k$}
	\Output{Clusters $A_1, \dots,A_k$ with $A_i = \{j|y_i \in C_i\}$ }
	\BlankLine
	\begin{enumerate}
		\item Construct a similarity graph. Let \textbf{W} be its weighted adjacency matrix
		\item Compute the unnormalized Laplacian \textbf{L}.
		\item Compute the first $k$ eigenvectors $u_1,\dots,u_k$ of \textbf{L}. Let $U \in \mathbb{R}^{n \times k}$ be the matrix containing the vectors $u_1,\dots,u_k$ as columns.
		\item \textbf{for} $i = 1,\dots,n$, let $y_i$ be the vector corresponding to the $i-th$ row of $U$
		\item Cluster the points $(y_i)_{i=1,\dots,n}$ in $\mathbb{R}^k$ with the K-Means algorithm into clusters $C_1,\dots,C_k$
	\end{enumerate}
	\caption{Spectral clustering}
	\label{alg:agglc}
\end{algorithm}\DecMargin{1em}

\subsection{Affinity Propagation}

\cite{Brusco2008}

\pagebreak

\section{Clustering Evaluation}
\label{sec:ce}
In this section we formally derive and explain the applied clustering metrics. Evaluating clustering performance has some issues. The algorithm assigns each point to a cluster. Despite we generated the "true" clusters the might not be comparable. A simple example we might consider the following situation. Let $y$ denote the labels of data the data and $y'$ the corresponding prediction such that  $y,y' \in \{0,1\}$. In a small example let our data points be like $y=(1,1,0,0)$ and the corresponding prediction $y'=(0,0,1,1)$. Obviously the clustering worked perfectly, however comparing "labels" would produce an accuracy of zero. 

There a several clustering metrics, which respect such a situation. We consider a bunch of information based metrics. Most of the measures use some sort of entropy. The following concepts can be found in \cite{Rosenberg2007} and  \cite{Vinh2010}. Additional reading is \cite{Hubert1985}. 

First we might introduce the contingency table. 

\setlength{\tabcolsep}{0.2cm}
\renewcommand{\arraystretch}{1}
\begin{table}[htb]
	\centering
	\begin{tabular}{c | c c c c| c}
		 & $V_1$ & $V_2$ & $\dots$ & $V_c$ & $\Sigma$ \\
		\hline
		$U_1$ & $n_{1,1}$ &$n_{1,2}$  &$\dots$ & & $a_1$ \\ 
		$U_2$ & $n_{2,1}$ & $\ddots$ & & & $a_1$ \\ 
		$\vdots$ & $n_{1,1}$ & $\dots$ & & & $a_1$ \\ 
		$U_R$ & $n_{1,1}$ & $\dots$ & & & $a_1$ \\ 
		\hline
		& $b_1$ & $b_2$ & $\dots$ & $b_c$ & N
	\end{tabular}
\caption{Contigency Table}
\end{table}

\begin{tabbing}
	\hspace*{1cm}\=\hspace*{1cm}\=\hspace*{3cm}\=\hspace*{2.7cm}\= \kill
	\onehalfspacing
	\textbf{$N_{11}$:} \>\> Number of pairs in the same cluster \\ 
	\textbf{$N_{00}$:} \>\> Number of pairs that are in different clusters in both $v$ and $u$ \\ 
	\textbf{$N_{01}$:} \>\> Number of pairs that are in the same cluster in both $u$ but different in $v$ \\ 
	\textbf{$N_{10}$:} \>\>  Number of pairs that are in the same cluster in both $v$ but different in $u$ \\ 
\end{tabbing}

\begin{flalign}
RI(u,v) = \frac{N_{00}+N_{11}}{\binom{N}{2}}
\label{eq:ri}
\end{flalign}

\begin{flalign}
ARI(u,v) = \frac{2 \left(N_{00}N_{11} - N_{01} N_{10} \right)}{\left(N_{00} + N_{01} \right)\left( N_{01} + N_{11}\right) + \left(N_{00} + N_{10} \right)\left( N_{10} + N_{11}\right)}
\label{eq:ri}
\end{flalign}

\begin{flalign}
H(u) &= - \sum_{i=1}^{R} \frac{a_i}{N} \log  \frac{a_i}{N} \\
H(v) &= - \sum_{i=1}^{C} \frac{b_i}{N} \log  \frac{a_i}{N} \\
H(u,v) &= - \sum_{i=1}^{R}  \sum_{j=1}^{C}  \frac{n_{i,j}}{N} \log \frac{n_{i,j}}{N} \\ 
H(u|v) &= - \sum_{i=1}^{R}  \sum_{j=1}^{C}  \frac{n_{i,j}}{N} \log \frac{n_{i,j}/N}{b_j/N} \\
H(v|u) &= - \sum_{i=1}^{C}  \sum_{j=1}^{R}  \frac{n_{i,j}}{N} \log \frac{n_{i,j}/N}{b_j/N} \\
I(u,v) &= \sum_{i=1}^{R}  \sum_{j=1}^{C}  \frac{n_{i,j}}{N} \log \frac{n_{i,j}/N}{a_i b_j/N} \\
\end{flalign}

Normalized Info Score:

This is one example of a normalized version 
\begin{flalign}
NMI_{max}(u,v) = \frac{I(u,v)}{\max\left( H(u),H(v)\right)}
\end{flalign}


\begin{flalign}
AMI_{max}(u,v) &= \frac{NMI_{max}(u,v) - \mathbb{E}\left[NMI_{max}(u,v)\right] }{1-\mathbb{E}\left[ NMI_{max}(u,v)\right]} \nonumber \\ 
&=\frac{I(u,v) - \mathbb{E}\left[ I(u,v)\right]}{ \max \left(H(u), H(v)\right) - \mathbb{E}\left[ I(u,v)\right] }
\end{flalign}

\begin{flalign}
\mathbb{E}\left[I(u,v) \right] = \sum_{i=1}^{R}  \sum_{j=1}^{C} \sum_{ n_{i,j}=\max \left(a_i + b_j-N,0\right) }^{ \min \left( a,b \right) } \frac{n_{ij}}{N} \log \left( \frac{N n_{ij}}{a_i b_j} \right) \frac{a_i! b_j!(N-a_i)!(N-b_j)!}{N! n_{ij}! (a_i-n_{ij})(b_j - n_{ij})!(N-a_i-b_j+n_{ij})!}
\end{flalign}

Homogeneity: 

\begin{flalign}
h = \begin{cases} 
1 & \text{,if } H(u,v) = 0 \\
1 - \frac{H(u|v)} {H(u)} & \text{otherwise}
\end{cases}
\end{flalign}

Completeness:
\begin{flalign}
c = \begin{cases} 
1 & \text{,if } H(v,u) = 0 \\
1 - \frac{H(v|u)} {H(v)} & \text{otherwise}
\end{cases}
\end{flalign}

V Measure Score:

\begin{flalign}
V_\beta = \frac{(1+\beta)h c}{\beta h + c}
\end{flalign}

\section{Simulation Data}

\begin{table}[H] \centering 
	\label{} 
	\scriptsize
	\begin{tabularx}{\textwidth}{ ccccccccccc} 
		\\\toprule
		Tau & Method & Similarity & C & BBC & E & EB & CC & NMI & ARI & VM \\ 
		\hline \\[-1.8ex] 
		\{0.1, 0.3\} & Spectral & DTW &  &  &  & x &  & 0.136 (0.134) & 0.094 (0.146) & 0.135 (0.134) \\ 
		& Spectral & EDR &  &  & x &  &  & 0.134 (0.135) & 0.097 (0.130) & 0.134 (0.135) \\ 
		& Spectral & Levensthein & x &  &  &  &  & 0.093 (0.134) & 0.050 (0.128) & 0.092 (0.134) \\ 
		& Spectral & Cosine &  & x &  &  &  & 0.113 (0.134) & 0.082 (0.150) & 0.113 (0.134) \\ 
		& K-Means & Euclidean &  &  &  &  & x & 0.093 (0.135) & 0.050 (0.130) & 0.092 (0.135) \\ 
		\{0.1, 0.5\} & Spectral & RBF &  &  &  & x &  & 0.331 (0.185) & 0.328 (0.208) & 0.331 (0.186) \\ 
		& Spectral & RBF &  &  & x &  &  & 0.322 (0.197) & 0.301 (0.221) & 0.321 (0.197) \\ 
		& Spectral & EDR &  &  & x &  &  & 0.315 (0.189) & 0.328 (0.206) & 0.315 (0.189) \\ 
		& K-Means & Euclidean &  &  & x &  &  & 0.313 (0.187) & 0.261 (0.213) & 0.312 (0.187) \\ 
		& Spectral & EDR &  & x &  &  &  & 0.226 (0.127) & 0.131 (0.113) & 0.223 (0.127) \\ 
		& Spectral & Levensthein & x &  &  &  &  & 0.219 (0.144) & 0.126 (0.121) & 0.217 (0.144) \\ 
		& K-Means & Euclidean &  &  &  &  & x & 0.208 (0.143) & 0.124 (0.122) & 0.207 (0.143) \\ 
		\{0.1, 0.7\} & Spectral & RBF &  &  &  & x &  & 0.642 (0.200) & 0.653 (0.206) & 0.641 (0.200) \\ 
		& Spectral & DTW &  &  & x &  &  & 0.580 (0.209) & 0.611 (0.205) & 0.580 (0.201) \\ 
		& K-Means & Euclidean &  &  & x &  &  & 0.533 (0.248) & 0.539 (0.275) & 0.533 (0.249) \\ 
		& K-Means & Euclidean &  &  &  &  & x & 0.382 (0.181) & 0.309 (0.237) & 0.381 (0.180) \\ 
		& Spectral & Levensthein & x &  &  &  &  & 0.350 (0.114) & 0.241 (0.163) & 0.347 (0.116) \\ 
		& Spectral & Euclidean &  & x &  &  &  & 0.306 (0.155) & 0.198 (0.179) & 0.303 (0.156) \\ 
		\{0.1, 1\} & Spectral & DTW &  &  &  & x &  & 0.938 (0.131) & 0.942 (0.126) & 0.938 (0.131) \\ 
		& Spectral & Euclidean &  &  &  & x &  & 0.811 (0.136) & 0.833 (0.129) & 0.811 (0.136) \\ 
		& Ward  & Euclidean &  &  &  & x &  & 0.797 (0.205) & 0.796 (0.218) & 0.797 (0.205) \\ 
		& Spectral & DTW &  &  & x &  &  & 0.765 (0.197) & 0.791 (0.184) & 0.765 (0.197) \\ 
		& K-Means & Euclidean &  &  & x &  &  & 0.757 (0.180) & 0.774 (0.185) & 0.757 (0.180) \\ 
		& K-Means & Euclidean &  &  &  &  & x & 0.796 (0.218) & 0.687 (0.265) & 0.696 (0.219) \\ 
		& Spectral & Overlap & x &  &  &  &  & 0.600 (0.299) & 0.571 (0.353) & 0.598 (0.302) \\ 
		& Spectral & Euclidean &  & x &  &  &  & 0.335 (0.091) & 0.220 (0.138) & 0.332 (0.093) \\ 
		\hline \\
	\end{tabularx} 
	\caption{Simulation results (Setting: Rounds = 20, Size = 20, $\mu = \{0,2,4\}$, $\sigma =  \{1,1,1\}$, $\alpha = 0.1$)} 
	\label{tab:apstab1}
\end{table} 

\begin{table}[H] \centering 
	\label{} 
	\scriptsize
		\begin{tabularx}{\textwidth}{ lllcccccccc} \\
			\toprule
			Tau & Method & Similarity & C & BBC & E & EB & CC & NMI & ARI & VM \\ 
			\hline \\[-1.8ex] 
		\{0.1, 0.3\} & Spectral & DTW &  &  &  & x &  & 0.136 (0.134) & 0.094 (0.146) & 0.135 (0.134) \\ 
		& Spectral & EDR &  &  & x &  &  & 0.134 (0.135) & 0.097 (0.130) & 0.134 (0.135) \\ 
		& Spectral & Levensthein & x &  &  &  &  & 0.093 (0.134) & 0.050 (0.128) & 0.092 (0.134) \\ 
		& Spectral & Cosine &  & x &  &  &  & 0.113 (0.134) & 0.082 (0.150) & 0.113 (0.134) \\ 
		& K-Means & Euclidean &  &  &  &  & x & 0.093 (0.135) & 0.050 (0.130) & 0.092 (0.135) \\ 
		\{0.1, 0.5\} & Spectral & RBF &  &  &  & x &  & 0.331 (0.185) & 0.328 (0.208) & 0.331 (0.186) \\ 
		& Spectral & RBF &  &  & x &  &  & 0.322 (0.197) & 0.301 (0.221) & 0.321 (0.197) \\ 
		& Spectral & EDR &  &  & x &  &  & 0.315 (0.189) & 0.328 (0.206) & 0.315 (0.189) \\ 
		& K-Means & Euclidean &  &  & x &  &  & 0.313 (0.187) & 0.261 (0.213) & 0.312 (0.187) \\ 
		& Spectral & EDR &  & x &  &  &  & 0.226 (0.127) & 0.131 (0.113) & 0.223 (0.127) \\ 
		& Spectral & Levensthein & x &  &  &  &  & 0.219 (0.144) & 0.126 (0.121) & 0.217 (0.144) \\ 
		& K-Means & Euclidean &  &  &  &  & x & 0.208 (0.143) & 0.124 (0.122) & 0.207 (0.143) \\ 
		\{0.1, 0.7\} & Spectral & RBF &  &  &  & x &  & 0.642 (0.200) & 0.653 (0.206) & 0.641 (0.200) \\ 
		& Spectral & DTW &  &  & x &  &  & 0.580 (0.209) & 0.611 (0.205) & 0.580 (0.201) \\ 
		& K-Means & Euclidean &  &  & x &  &  & 0.533 (0.248) & 0.539 (0.275) & 0.533 (0.249) \\ 
		& K-Means & Euclidean &  &  &  &  & x & 0.382 (0.181) & 0.309 (0.237) & 0.381 (0.180) \\ 
		& Spectral & Levensthein & x &  &  &  &  & 0.350 (0.114) & 0.241 (0.163) & 0.347 (0.116) \\ 
		& Spectral & Euclidean &  & x &  &  &  & 0.306 (0.155) & 0.198 (0.179) & 0.303 (0.156) \\ 
		\{0.1, 1\} & Spectral & DTW &  &  &  & x &  & 0.938 (0.131) & 0.942 (0.126) & 0.938 (0.131) \\ 
		& Spectral & Euclidean &  &  &  & x &  & 0.811 (0.136) & 0.833 (0.129) & 0.811 (0.136) \\ 
		& Ward  & Euclidean &  &  &  & x &  & 0.797 (0.205) & 0.796 (0.218) & 0.797 (0.205) \\ 
		& Spectral & DTW &  &  & x &  &  & 0.765 (0.197) & 0.791 (0.184) & 0.765 (0.197) \\ 
		& K-Means & Euclidean &  &  & x &  &  & 0.757 (0.180) & 0.774 (0.185) & 0.757 (0.180) \\ 
		& K-Means & Euclidean &  &  &  &  & x & 0.796 (0.218) & 0.687 (0.265) & 0.696 (0.219) \\ 
		& Spectral & Overlap & x &  &  &  &  & 0.600 (0.299) & 0.571 (0.353) & 0.598 (0.302) \\ 
		& Spectral & Euclidean &  & x &  &  &  & 0.335 (0.091) & 0.220 (0.138) & 0.332 (0.093) \\ 
		\hline \\[-1.8ex] 
		\end{tabularx} 
			\caption{Simulation results (Setting: Rounds = 20, Size = 20, $\mu = \{0,2,4\}$, $\sigma =  \{1,1,1\}$, $\alpha = 0.5$)} 
			\label{tab:apstab2}	
		\end{table}
	
\begin{table}[H] \centering 
	\label{} 
	\scriptsize
		\begin{tabularx}{\textwidth}{ lllcccccccc} \\
		\\[-1.8ex]	\toprule
	 \\[-1.8ex] 
		Tau & Method & Similarity & C & BBC & E & EB & CC & NMI & ARI & VM \\ 
		\hline \\[-1.8ex] 
		\{0.1, 0.3\} & Spectral  & Overlap & x &  &  &  &  & 0.103 (0.019) & 0.002 (0.007) & 0.087 (0.020) \\ 
		& Spectral  & EDR &  & x &  &  &  & 0.096 (0.061) & 0.016 (0.049) & 0.088 (0.059) \\ 
		& Spectral  & EDR &  &  &  & x &  & 0.068 (0.080) & 0.008 (0.085) & 0.063 (0.078) \\ 
		& Spectral  & Cosine &  &  & x &  &  & 0.043 (0.067) & - & 0.043 (0.067) \\ 
		& K-Means & Euclidean  &  &  &  &  & x & 0.036 (0.041) &  -  & 0.036 (0.041) \\ 
		\{0.1, 0.5\} & Average & Euclidean  &  & x &  &  &  & 0.133 (0.115) & 0.040 (0.077) & 0.127 (0.115) \\ 
		& K-Means & Euclidean  & x &  &  &  &  & 0.112 (0.124) & 0.035 (0.085) & 0.110 (0.122) \\ 
		& Spectral  & Euclidean  &  &  &  & x &  & 0.103 (0.096) & 0.060 (0.092) & 0.103 (0.095) \\ 
		& Spectral  & Cosine &  &  & x &  &  & 0.080 (0.088) & 0.038 (0.092) & 0.080 (0.088) \\ 
		& K-Means & Euclidean  &  &  &  &  & x & 0.079 (0.097) & 0.034 (0.088) & 0.079 (0.096) \\ 
		\{0.1, 0.7\} & Spectral  & DTW &  &  &  & x &  & 0.361 (0.200) & 0.352 (0.211) & 0.361 (0.200) \\ 
		& Spectral  & EDR &  &  &  & x &  & 0.223 (0.083) & 0.087 (0.080) & 0.217 (0.086) \\ 
		& Ward & Euclidean  &  &  &  & x &  & 0.195 (0.143) & 0.103 (0.135) & 0.192 (0.143) \\ 
		& K-Means & Euclidean  &  & x &  &  &  & 0.167 (0.123) & 0.064 (0.114) & 0.161 (0.123) \\ 
		& K-Means & Euclidean  & x &  &  &  &  & 0.157 (0.127) & 0.060 (0.116) & 0.151 (0.127) \\ 
		& Spectral  & DTW &  &  & x &  &  & 0.105 (0.140) & 0.087 (0.179) & 0.105 (0.140) \\ 
		& K-Means & Euclidean  &  &  &  &  & x & 0.102 (0.183) & 0.063 (0.201) & 0.101 (0.183) \\ 
		\{0.1, 1.0\} & Spectral  & DTW &  &  &  & x &  & 0.792 (0.174) & 0.808 (0.170) & 0.792 (0.174) \\ 
		& Spectral  & RBF &  &  &  & x &  & 0.692 (0.164) & 0.692 (0.184) & 0.692 (0.164) \\ 
		& K-Means & Euclidean  &  &  &  & x &  & 0.606 (0.188) & 0.578 (0.233) & 0.606 (0.189) \\ 
		& Spectral  & EDR &  & x &  &  &  & 0.280 (0.198) & 0.201 (0.215) & 0.278 (0.198) \\ 
		& Spectral  & Levensthein & x &  &  &  &  & 0.276 (0.101) & 0.154 (0.113) & 0.272 (0.111) \\ 
		& Spectral  & EDR &  &  & x &  &  & 0.236 (0.196) & 0.171 (0.184) & 0.235 (0.195) \\ 
		\hline \\[-1.8ex] 
	\end{tabularx} 
	\caption{Simulation results (Setting: Rounds = 20, Size = 20, $\mu = \{0,2,4\}$, $\sigma =  \{1,1,1\}$, $\alpha = 1$)} 
		\label{tab:apstab3}
\end{table} 



% Table created by stargazer v.5.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu
% Date and time: Sat, Jun 25, 2016 - 12:04:36
\begin{table}[H] \centering 
	\label{} 
	\scriptsize
	\begin{tabularx}{\textwidth}{ lllcccccccc} \\
		\\[-1.8ex]	\toprule
		\\[-1.8ex] 
		Tau & Method & Similarity & C & BBC & E & EB & CC & NMI & ARI & VM \\ 
		\hline \\[-1.8ex] 
		\{0.1, 0.3\} & Spectral & RBF &  &  &  & x &  & 0.290 (0.173) & 0.279 (0.205) & 0.279 (0.173) \\ 
		& Spectral & EDR &  &  & x &  &  & 0.284 (0.253) & 0.279 (0.274) & 0.284 (0.253) \\ 
		& Spectral & Levensthein & x &  &  &  &  & 0.275 (0.147) & 0.195 (0.156) & 0.272 (0.148) \\ 
		& K-Means & Euclidean &  &  &  &  & x & 0.260 (0.126) & 0.200 (0.150) & 0.258 (0.127) \\ 
		& Spectral & Euclidean &  & x &  &  &  & 0.215 (0.144) & 0.126 (0.159) & 0.211 (0.144) \\ 
		\{0.1, 0.5\} & Spectral & DTW &  &  &  & x &  & 0.824 (0.166) & 0.837 (0.170) & 0.824 (0.166) \\ 
		& Spectral & DTW &  &  & x &  &  & 0.636 (0.213) & 0.661 (0.205) & 0.636 (0.213) \\ 
		& K-Means & Euclidean &  &  &  &  & x & 0.532 (0.182) & 0.543 (0.206) & 0.531 (0.182) \\ 
		& Spectral & Levensthein & x &  &  &  &  & 0.411 (0.125) & 0.333 (0.160) & 0.410 (0.127) \\ 
		& Spectral & Euclidean &  & x &  &  &  & 0.242 (0.163) & 0.132 (0.187) & 0.238 (0.165) \\ 
		\{0.1, 0.7\} & Spectral & DTW &  &  &  & x &  & 1.00 (0.000) & 1.00 (0.000) & 1.00 (0.000) \\ 
		& Spectral & DTW &  &  & x &  &  & 0.726 (0.228) & 0.750 (0.214) & 0.726 (0.228) \\ 
		& K-Means & Euclidean &  &  &  &  & x & 0.721 (0.211) & 0.741 (0.207) & 0.721 (0.211) \\ 
		& Spectral & Levensthein & x &  &  &  &  & 0.333 (0.147) & 0.222 (0.195) & 0.329 (0.150) \\ 
		\hline \\[-1.8ex] 
	\end{tabularx} 
	\caption{Simulation results (Setting: Rounds = 20, Size = 20, $\mu = \{0,1,2\}$, $\sigma =  \{1,1,1\}$, $\alpha = 0.1$)} 
		\label{tab:apstab4}
\end{table} 


\begin{table}[H] \centering 
	\label{} 
	\scriptsize
	\begin{tabularx}{\textwidth}{ lllcccccccc} \\
		\\[-1.8ex]	\toprule
		\\[-1.8ex] 
		Tau & Method & Similarity & C & BBC & E & EB & CC & NMI & ARI & VM \\ 
		\hline \\[-1.8ex] 
\{0.1, 0.3\} & Average & Euclidean &  & x &  &  &  & 0.101 (0.051) & 0.005 (0.023) & 0.088 (0.050) \\ 
& Average & Euclidean &  &  &  & x &  & 0.093 (0.060) & 0.006 (0.024) & 0.084 (0.057) \\ 
& Spectral & Overlap & x &  &  &  &  & 0.091 (0.073) & 0.007 (0.029) & 0.084 (0.070) \\ 
& Spectral & EDR &  &  & x &  &  & 0.071 (0.077) & 0.025 (0.072) & 0.070 (0.076) \\ 
& K-Means & Euclidean &  &  &  &  & x & 0.057 (0.064) & 0.014 (0.069) & 0.057 (0.064) \\ 
\{0.1, 0.5\} & Spectral & RBF &  &  &  & x &  & 0.256 (0.190) & 0.221 (0.195) & 0.255 (0.190) \\ 
& Spectral & Overlap & x &  &  &  &  & 0.092 (0.079) & 0.008 (0.041) & 0.087 (0.075) \\ 
& Ward & Euclidean &  & x &  &  &  & 0.092 (0.085) & 0.007 (0.040) & 0.088 (0.082) \\ 
& K-Means & Euclidean &  &  & x &  &  & 0.080 (0.100) & 0.043 (0.122) & 0.079 (0.100) \\ 
& K-Means & Euclidean &  &  &  &  & x & 0.062 (0.093) & 0.026 (0.116) & 0.062 (0.093) \\ 
\{0.1, 0.7\} & Spectral & DTW &  &  &  & x &  & 0.618 (0.192) & 0.627 (0.201) & 0.617 (0.192) \\ 
& Spectral & Levensthein & x &  &  &  &  & 0.252 (0.137) & 0.163 (0.147) & 0.249 (0.137) \\ 
& Spectral & RBF &  &  & x &  &  & 0.215 (0.158) & 0.207 (0.188) & 0.215 (0.157) \\ 
& Spectral & DTW &  & x &  &  &  & 0.205 (0.218) & 0.192 (0.247) & 0.205 (0.218) \\ 
& K-Means & Euclidean &  &  &  &  & x & 0.185 (0.205) & 0.165 (0.225) & 0.185 (0.205) \\ 
\{0.1, 1.0\} & Spectral & RBF &  &  &  & x &  & 0.952 (0.098) & 0.960 (0.082) & 0.952 (0.098) \\ 
& Spectral & DTW &  & x &  &  &  & 0.489 (0.269) & 0.491 (0.290) & 0.488 (0.269) \\ 
& K-Means & Euclidean &  &  &  &  & x & 0.453 (0.214) & 0.427 (0.252) & 0.452 (0.215) \\ 
& Spectral & DTW &  &  & x &  &  & 0.406 (0.203) & 0.403 (0.226) & 0.406 (0.203) \\ 
		\hline \\[-1.8ex] 
	\end{tabularx} 
		\caption{Simulation results (Setting: Rounds = 20, Size = 20, $\mu = \{0,1,2\}$, $\sigma =  \{1,1,1\}$, $\alpha = 1$)} 
			\label{tab:apstab5}
\end{table} 

\begin{table}[H] \centering 
	\label{} 
	\scriptsize
	\begin{tabularx}{\textwidth}{ lllcccccccc} \\
		\\[-1.8ex]	\toprule
		\\[-1.8ex] 
Tau & Method & Similarity & C & BBC & E & EB & CC & NMI & ARI & VM \\ 
\hline \\[-1.8ex] 
\{0.1,0.3\} & Spectral & Cosine &  &  & x &  &  & 0.088 (0.116) & 0.038 (0.105) & 0.088 (0.115) \\ 
& Spectral & Cosine & x &  &  &  &  & 0.087 (0.099) & 0.019 (0.075) & 0.084 (0.097) \\ 
& Average & Euclidean &  &  &  & x &  & 0.083 (0.107) & 0.026 (0.095) & 0.080 (0.106) \\ 
& Spectral & Cosine &  & x &  &  &  & 0.039 (0.055) & 0.000 (0.068) & 0.039 (0.055) \\ 
& K-Means & Euclidean &  &  &  &  & x & 0.037 (0.057) & -0 & 0.037 (0.056) \\ 
\{0.1,0.5\} & Spectral & EDR &  &  &  & x &  & 0.088 (0.087) & 0.027 (0.078) & 0.083 (0.086) \\ 
& Spectral & Overlap & x &  &  &  &  & 0.066 (0.051) & 0.026 (0.062) & 0.065 (0.050) \\ 
& Spectral & Cosine &  &  & x &  &  & 0.047 (0.064) & 0.002 (0.067) & 0.047 (0.063) \\ 
& Spectral & DTW &  & x &  &  &  & 0.030 (0.045) & -0 & 0.030 (0.045) \\ 
& K-Means & Euclidean &  &  &  &  & x & 0.029 (0.045) & -0 & 0.029 (0.045) \\ 
\{0.1,0.7\} & Average & Euclidean &  &  &  & x &  & 0.087 (0.038) & 0.007 (0.039) & 0.076 (0.034) \\ 
& Spectral & Cosine & x &  &  &  &  & 0.081 (0.076) & 0.013 (0.044) & 0.078 (0.072) \\ 
& K-Means & Euclidean &  &  & x &  &  & 0.070 (0.074) & 0.014 (0.061) & 0.068 (0.072) \\ 
& Spectral & EDR &  & x &  &  &  & 0.048 (0.054) & 0.003 (0.063) & 0.047(0.053) \\ 
& K-Means & Euclidean &  &  &  &  & x & 0.041 (0.071) & -0 & 0.041 (0.071) \\ 
\{0.1,0.9\} & Spectral & Cosine & x &  &  &  &  & 0.180 (0.110) & 0.069 (0.081) & 0.175 (0.109) \\ 
& Spectral & EDR &  & x &  &  &  & 0.161 (0.125) & 0.097 (0.129) & 0.159 (0.124) \\ 
& K-Means & Euclidean &  &  & x &  &  & 0.142 (0.127) & 0.049 (0.097) & 0.139 (0.126) \\ 
& Average & Euclidean &  &  &  & x &  & 0.115 (0.034) & 0.008 (0.016) & 0.100 (0.037) \\ 
& K-Means & Euclidean &  &  &  &  & x & 0.058 (0.108) & 0.016 (0.115) & 0.058 (0.108) \\ 
\{0.5, 0.9\} & Spectral & Cosine & x &  &  &  &  & 0.169 (0.125) & 0.075 (0.097) & 0.164 (0.126) \\ 
& Spectral & EDR &  &  & x &  &  & 0.158 (0.184) & 0.111 (0.188) & 0.157 (0.184) \\ 
& Spectral & EDR &  & x &  &  &  & 0.150 (0.138) & 0.103 (0.149) & 0.148 (0.137) \\ 
& Average & Euclidean &  &  &  & x &  & 0.104 (0.040) & 0.003 (0.014) & 0.091 (0.038) \\ 
\hline \\[-1.8ex] 
	\end{tabularx} 
	\caption{Simulation results (Setting: Rounds = 20, Size = 20, $\mu = \{0,2,4\}$, $\sigma =  \{1,1,1\}$, $\tau = 0.1$)} 
		\label{tab:apstab6}
\end{table} 

\begin{table}[H] \centering 
	\label{} 
	\scriptsize
	\begin{tabularx}{\textwidth}{ lllcccccccc} \\
		\\[-1.8ex]	\toprule
		\\[-1.8ex] 
		Alpha & Method & Similarity & C & BBC & E & EB & CC & NMI & ARI & VM \\ 
		\hline \\[-1.8ex] 
	\{0.1,0.3\} & Spectral & Cosine &  &  &  & x &  & 0.131 (0.114) & 0.100 (0.133) & 0.131 (0.113) \\ 
	& K-Means & Euclidean & x &  &  &  &  & 0.130 (0.199) & 0.075 (0.212) & 0.127 (0.199) \\ 
	& Spectral & Cosine &  & x &  &  &  & 0.119 (0.104) & 0.068 (0.094) & 0.118 (0.103) \\ 
	& K-Means & Euclidean &  &  &  &  & x & 0.113 (0.156) & 0.089 (0.181) & 0.113 (0.156) \\ 
	& Spectral & RBF &  &  & x &  &  & 0.101 (0.140) & 0.073 (0.162) & 0.101 (0.140) \\ 
	\{0.1,0.5\} & Spectral & Cosine &  &  & x &  &  & 0.199 (0.131) & 0.141 (0.129) & 0.198 (0.131) \\ 
	& Spectral & Cosine &  & x &  &  &  & 0.191 (0.157) & 0.128 (0.148) & 0.190 (0.157) \\ 
	& Spectral & Euclidean &  &  &  & x &  & 0.168 (0.183) & 0.134 (0.186) & 0.168 (0.182) \\ 
	& K-Means & Euclidean &  &  &  &  & x & 0.122 (0.121) & 0.085 (0.122) & 0.121 (0.121) \\ 
	& Spectral & Cosine & x &  &  &  &  & 0.063 (0.081) & 0.033 (0.109) & 0.063 (0.081) \\ 
	\{0.1,0.7\} & Spectral & Cosine &  & x &  &  &  & 0.277 (0.229) & 0.222 (0.240) & 0.276 (0.229) \\ 
	& Spectral & Cosine &  &  &  & x &  & 0.278 (0.272) & 0.183 (0.254) & 0.271 (0.183) \\ 
	& K-Means & Euclidean &  &  & x &  &  & 0.227 (0.178) & 0.186 (0.176) & 0.227 (0.177) \\ 
	& Spectral & Cosine & x &  &  &  &  & 0.190 (0.114) & 0.152 (0.116) & 0.189 (0.114) \\ 
	& K-Means & Euclidean &  &  &  &  & x & 0.178 (0.150) & 0.147 (0.147) & 0.177 (0.149) \\ 
	\{0.1,0.9\} & Spectral & DTW &  &  &  & x &  & 0.300 (0.232) & 0.273 (0.254) & 0.299 (0.232) \\ 
	& Spectral & Cosine &  & x &  &  &  & 0.211 (0.158) & 0.159 (0.160) & 0.210 (0.158) \\ 
	& Spectral & Cosine &  &  & x &  &  & 0.191 (0.117) & 0.137 (0.127) & 0.189 (0.118) \\ 
	& Spectral & Cosine & x &  &  &  &  & 0.164 (0.110) & 0.115 (0.109) & 0.162 (0.109) \\ 
	& K-Means & Euclidean &  &  &  &  & x & 0.138 (0.153 & 0.091 (0.176) & 0.137 (0.153) \\
	\hline \\[-1.8ex] 
	\end{tabularx} 
	\caption{Simulation results (Setting: Rounds = 20, Size = 20, $\mu = \{0,2,4\}$, $\sigma =  \{1,1,1\}$, $\tau = 0.5$)} 
		\label{tab:apstab7}
\end{table} 

\begin{table}[H] \centering 
	\label{} 
	\scriptsize
	\begin{tabularx}{\textwidth}{ lllcccccccc} \\
		\\[-1.8ex]	\toprule
		\\[-1.8ex] 
		Alpha & Method & Similarity & C & BBC & E & EB & CC & NMI & ARI & VM \\ 
		\hline \\[-1.8ex] 
\{0.1, 0.3\} & Spectral  & Overlap & x &  &  &  &  & 0.103 (0.019) & 0.002 (0.007) & 0.087 (0.020) \\ 
& Spectral  & EDR &  & x &  &  &  & 0.096 (0.061) & 0.016 (0.049) & 0.088 (0.059) \\ 
& Spectral  & EDR &  &  &  & x &  & 0.068 (0.080) & 0.008 (0.085) & 0.063 (0.078) \\ 
& Spectral  & Cosine &  &  & x &  &  & 0.043 (0.067) & -0 & 0.043 (0.067) \\ 
& K-Means & Euclidean  &  &  &  &  & x & 0.036 (0.041) &  -  & 0.036 (0.041) \\ 
\{0.1, 0.5\} & Average & Euclidean  &  & x &  &  &  & 0.133 (0.115) & 0.040 (0.077) & 0.127 (0.115) \\ 
& K-Means & Euclidean  & x &  &  &  &  & 0.112 (0.124) & 0.035 (0.085) & 0.110 (0.122) \\ 
& Spectral  & Euclidean  &  &  &  & x &  & 0.103 (0.096) & 0.060 (0.092) & 0.103 (0.095) \\ 
& Spectral  & Cosine &  &  & x &  &  & 0.080 (0.088) & 0.038 (0.092) & 0.080 (0.088) \\ 
& K-Means & Euclidean  &  &  &  &  & x & 0.079 (0.097) & 0.034 (0.088) & 0.079 (0.096) \\ 
\{0.1, 0.7\} & Spectral  & DTW &  &  &  & x &  & 0.361 (0.200) & 0.352 (0.211) & 0.361 (0.200) \\ 
& Spectral  & EDR &  &  &  & x &  & 0.223 (0.083) & 0.087 (0.080) & 0.217 (0.086) \\ 
& Ward & Euclidean  &  &  &  & x &  & 0.195 (0.143) & 0.103 (0.135) & 0.192 (0.143) \\ 
& K-Means & Euclidean  &  & x &  &  &  & 0.167 (0.123) & 0.064 (0.114) & 0.161 (0.123) \\ 
& K-Means & Euclidean  & x &  &  &  &  & 0.157 (0.127) & 0.060 (0.116) & 0.151 (0.127) \\ 
& Spectral  & DTW &  &  & x &  &  & 0.105 (0.140) & 0.087 (0.179) & 0.105 (0.140) \\ 
& K-Means & Euclidean  &  &  &  &  & x & 0.102 (0.183) & 0.063 (0.201) & 0.101 (0.183) \\ 
\{0.1, 1.0\} & Spectral  & DTW &  &  &  & x &  & 0.792 (0.174) & 0.808 (0.170) & 0.792 (0.174) \\ 
& Spectral  & RBF &  &  &  & x &  & 0.692 (0.164) & 0.692 (0.184) & 0.692 (0.164) \\ 
& K-Means & Euclidean  &  &  &  & x &  & 0.606 (0.188) & 0.578 (0.233) & 0.606 (0.189) \\ 
& Spectral  & EDR &  & x &  &  &  & 0.280 (0.198) & 0.201 (0.215) & 0.278 (0.198) \\ 
& Spectral  & Levensthein & x &  &  &  &  & 0.276 (0.101) & 0.154 (0.113) & 0.272 (0.111) \\ 
& Spectral  & EDR &  &  & x &  &  & 0.236 (0.196) & 0.171 (0.184) & 0.235 (0.195) \\ 
& K-Means & Euclidean  &  &  &  &  & x & 0.099 (0.105) & 0.052 (0.097) & 0.098 (0.104) \\ 
\hline \\[-1.8ex] 
	\end{tabularx} 
	\caption{Simulation results (Setting: Rounds = 20, Size = 20, $\mu = \{0,2,4\}$, $\sigma =  \{1,1,1\}$, $\tau = 1.0$)} 
		\label{tab:apstab8}
\end{table} 

\subsection{Prison data}

\begin{table}[!htbp] \centering 
	\caption{} 
	\label{} 
	\scriptsize 
	\begin{tabular}{@{\extracolsep{0pt}} llcccccccccc} 
		\toprule
		Groups & Method & Similarity & C & CBC & BBC & E & EB & CC & NMI & ARI & VM \\ 
		\hline \\[-1.8ex] 
		6vs1 & Spectral & EDR &  &  &  & x &  &  & 0.237 (0.000) & 0.020 (0.000) & 0.237 (0.000) \\ 
		& K-Means &  &  &  &  &  & x &  & 0.167 (0.000) & - & 0.166 (0.000) \\ 
		& Spectral & Euclidean &  & x &  &  &  &  & 0.057 (0.000) & 0.036 (0.000) & 0.057 (0.000) \\ 
		& Spectral & EDR &  &  & x &  &  &  & 0.037 (0.000) & 0.034 (0.000) & 0.037 (0.000) \\ 
		& K-Means & Euclidean &  &  &  &  &  & x & 0.023 (0.028) & - & 0.023 (0.028) \\ 
		& K-Means &  & x &  &  &  &  &  & 0.020 (0.022) & - & 0.020 (0.023) \\ 
		& Average &  &  &  &  &  &  &  & 0.068 (0.068) & - & 0.067 (0.068) \\ 
		6vs2 & K-Means &  &  & x &  &  &  &  & 0.289 (0.052) & 0.217 (0.063) & 0.278 (0.055) \\ 
		& K-Means &  &  &  &  & x &  &  & 0.165 (0.000) & 0.083 (0.000) & 0.145 (0.000) \\ 
		& Spectral & Cosine &  &  &  &  & x &  & 0.165 (0.000) & 0.083 (0.000) & 0.145 (0.000) \\ 
		& Ward & Euclidean &  &  & x &  &  &  & 0.072 (0.000) & 0.083 (0.000) & 0.071 (0.000) \\ 
		& K-Means &  & x &  &  &  &  &  & 0.041 (0.022) & - & 0.041 (0.022) \\ 
		& K-Means & Euclidean &  &  &  &  &  & x & 0.028 (0.023) & - & 0.028 (0.023) \\ 
		& Average &  &  &  &  &  &  &  & 0.116 (0.088) & 0.058 (0.081) & 0.110 (0.083) \\ 
		6vs3 & Spectral & warp &  &  &  & x &  &  & 0.205 (0.000) & - & 0.203 (0.000) \\ 
		& K-Means &  &  &  &  &  & x &  & 0.174 (0.000) & - & 0.171 (0.000) \\ 
		& Spectral & warp &  & x &  &  &  &  & 0.116 (0.000) & - & 0.110 (0.000) \\ 
		& Ward & Euclidean &  &  & x &  &  &  & 0.116 (0.000) & - & 0.110 (0.000) \\ 
		& Lavenstein &  & x &  &  &  &  &  & 0.116 (0.000) & - & 0.110 (0.000) \\ 
		& K-Means & Euclidean &  &  &  &  &  & x & 0.110 (0.023) & - & 0.105 (0.022) \\ 
		& Average &  &  &  &  &  &  &  & 0.089 (0.056) & - & 0.086 (0.055) \\ 
		6vs4 & Spectral & EDR &  &  &  & x &  &  & 0.360 (0.000) & 0.227 (0.000) & 0.342 (0.000) \\ 
		& Spectral & Euclidean  &  &  &  &  & x &  & 0.246 (0.000) & 0.229 (0.000) & 0.246 (0,000) \\ 
		& Ward & Euclidean  &  & x &  &  &  &  & 0.105 (0.000) & - & 0.102 (0.000) \\ 
		& Complete & Euclidean  &  &  & x &  &  &  & 0.848 (0.000) & - & 0.080 (0.000) \\ 
		& K-Means & Euclidean  & x &  &  &  &  &  & 0.049 (0.000) & - & 0.049 (0.000) \\ 
		& K-Means & Euclidean  &  &  &  &  &  & x & 0.080 (0.050) & - & 0.079 (0.048) \\ 
		& Average &  &  &  &  &  &  &  & 0.089 (0.088) & - & 0.088 (0.088) \\ 
		6vs9 & Ward  & Euclidean &  &  &  &  & x &  & 0.561 (0.000) & 0.560 (0,000) & 0.560 (0,000) \\ 
		& Spectral & DTW &  &  &  & x &  &  & 0.435 (0.000) & 0.387 (0.000) & 0.432 (0.000) \\ 
		& Spectral & DTW &  & x &  &  &  &  & 0.435 (0.000) & 0.387 (0.000) & 0.432 (0.000) \\ 
		& Spectral & EDR &  &  & x &  &  &  & 0.333 (0.000) & 0.381 (0.000) & 0.333 (0.000) \\ 
		& Spectral & Overlap & x &  &  &  &  &  & 0.111 (0.000) & 0.118 (0.000) & 0.111 (0.000) \\ 
		& K-Means & Euclidean &  &  &  &  &  & x & 0.106 (0.066) & 0.033 (0.054) & 0.094 (0.059) \\ 
		& Average &  &  &  &  &  &  &  & 0.249 (0.167) & 0.202 (0.189) & 0.244 (0.169) \\ 
		\hline \\[-1.8ex] 
	\end{tabular} 
	\label{tab:prison1}
\end{table} 

\begin{table}[!htbp] \centering 
	\caption{} 
	\label{} 
	\scriptsize 
	\begin{tabular}{@{\extracolsep{0pt}} cccccccccccc} 
		\toprule
		Groups & Method & Similarity & C & CBC & BBC & E & EB & CC & NMI & ARI & VM \\ 
		\hline \\[-1.8ex] 
		2vs4 & Spectral & eucsim/rbf &  & x &  &  &  &  & 0.213 (0.000) & 0.007 (0.000) & 0.211 (0.000) \\ 
		& Levenstein &  & x &  &  &  &  &  & 0.213 (0.000) & 0.007 (0.000) & 0.211 (0.000) \\ 
		& Spectral & EDR &  &  &  & x &  &  & 0.132 (0.000) & 0.104 (0.000) & 0.131 (0.000) \\ 
		& Spectral & EDR &  &  & x &  &  &  & 0.054 (0.000) & - & 0.054 (0.000) \\ 
		& Spectral & eblock &  &  &  &  & x &  & 0.018 (0.000) & - & 0.017 (0.000) \\ 
		& K-Means & Euclidean &  &  &  &  &  & x & 0.005 (0.005) & - & 0.005 (0.005) \\ 
		&  &  &  &  &  &  &  &  & 0.042 (0.063) & - & 0.041 (0.063) \\ 
		2vs6 & K-Means &  &  & x &  &  &  &  & 0.3 (0.048) & 0.23 (0.062) & 0.291 (0.510) \\ 
		& K-Means &  &  &  &  & x &  &  & 0.165 (0.000) & 0.083 (0.000) & 0.145 (0.000) \\ 
		& Spectral & Cosine &  &  &  &  & x &  & 0.165 (0.000) & 0.083 (0.000) & 0.145 (0.000) \\ 
		& Ward & Euclidean &  &  & x &  &  &  & 0.072 (0.000) & 0.083 (0.000) & 0.071 (0.000) \\ 
		& K-Means & Euclidean &  &  &  &  &  & x & 0.027 (0.020) & - & 0.027 (0.020) \\ 
		& K-Means &  & x &  &  &  &  &  & 0.023 (0.021) & - & 0.023 (0.021) \\ 
		&  &  &  &  &  &  &  &  & 0.112 (0.090) & 0.057 (0.082) & 0.106 (0.085) \\ 
		2vs9 & Spectral & EDR &  &  &  & x &  &  & 0.382 (0.000) & 0.235 (0.000) & 0.382 (0.000) \\ 
		& Spectral & EDR &  &  & x &  &  &  & 0.232 (0.000) & 0.226 (0.000) & 0.232 (0.000) \\ 
		& Spectral & Cosine &  &  &  &  & x &  & 0.197 (0.000) & 0.008 (0.000) & 0.191 (0.000) \\ 
		& Spectral & eucsim/rbf &  & x &  &  &  &  & 0.134 (0.000) & 0.075 (0.000) & 0.134 (0.000) \\ 
		& K-Means & Euclidean &  &  &  &  &  & x & 0.116 (0.000) & - & 0.105 (0.000) \\ 
		& K-Means &  & x &  &  &  &  &  & 0.111 (0.022) & - & 0.100 (0.020) \\ 
		&  &  &  &  &  &  &  &  & 0.096 (0.074) & - & 0.093 (0.073) \\ 
		\hline \\[-1.8ex] 
	\end{tabular} 
	\label{tab:prison2}
\end{table} 





\end{appendix}

\pagebreak
%----------------------------------------------------------------------------------------------------------
% End Document
%----------------------------------------------------------------------------------------------------------

\end{document}
